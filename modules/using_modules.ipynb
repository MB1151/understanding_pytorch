{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In this notebook, you learn:\n",
    "# \n",
    "# 1) How to use pytorch module to build custom models?\n",
    "# 2) How to use the Sequential module?\n",
    "# 3) What are submodules of a module?\n",
    "# 4) What is state dictionary of a module?\n",
    "# 5) What are Buffers of a module?\n",
    "# 6) How to use ModuleList?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn, Tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Official pytorch blog (https://pytorch.org/docs/stable/notes/modules.html) that explains about using module.\n",
    "# The examples below are directly copied from the official blog. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building Custom Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Refer to 'modules/understanding_nn_linear.ipynb' to understand how to use the Linear Layer from\n",
    "# pytorch built-in libraries.\n",
    "# We will now try to build the linear layer ourself but using the pytorch 'module' library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All custom modules should inherit from the pytorch 'module'. It provides various useful features\n",
    "# that help train the model.\n",
    "class CustomLinearLayer(nn.Module):\n",
    "    def __init__(self, num_in_features: int, num_out_features: int):\n",
    "        super().__init__()\n",
    "        # nn.Parameters are the learnable variables of the model. Pytorch tracks all the parameters\n",
    "        # by default and calculates the gradients for these parameters during back propagration.\n",
    "        self.weight = nn.Parameter(data=torch.randn(size=(num_in_features, num_out_features), dtype=torch.float), requires_grad=True)\n",
    "        # nn.Paramters are also added to the attribute 'paramters' that is maintained by the\n",
    "        # pytorch module class.\n",
    "        self.bias = nn.Parameter(data=torch.randn(size=(num_out_features,), dtype=torch.float), requires_grad=True)\n",
    "    \n",
    "    # The forward function can perform any arbitrary operation. Here, we are just performing a \n",
    "    # linear transformation.\n",
    "    def forward(self, input: Tensor):\n",
    "        # '@' operation performs matrix multiplication. Refer to 'understanding_tensor_manipulations_part_4.ipynb'\n",
    "        # to understand more about this operator.\n",
    "        return (input @ self.weight) + self.bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CustomLinearLayer()\n"
     ]
    }
   ],
   "source": [
    "linear_layer = CustomLinearLayer(num_in_features=5, num_out_features=2)\n",
    "print(linear_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1., 2., 3., 4., 5.])\n",
      "torch.Size([5])\n"
     ]
    }
   ],
   "source": [
    "sample_input_1 = torch.tensor(data=[1, 2, 3, 4, 5], dtype=torch.float)\n",
    "print(sample_input_1)\n",
    "print(sample_input_1.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 6.6635, -9.2504], grad_fn=<AddBackward0>)\n",
      "torch.Size([2])\n"
     ]
    }
   ],
   "source": [
    "# Module objects act as callables and calling it invokes the forward function.\n",
    "output_1 = linear_layer(sample_input_1)\n",
    "print(output_1)\n",
    "print(output_1.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sequential Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  (0): CustomLinearLayer()\n",
      "  (1): ReLU()\n",
      "  (2): CustomLinearLayer()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Modules can contain other modules to combine various functionalities. The simplest way to\n",
    "# combine multiple modules is the Sequential module.\n",
    "\n",
    "# The output of the current layer is directly fed into the next layer as input i.e., output\n",
    "# of first CustomLinearLayer is passed as input to the ReLU function. The output of ReLU \n",
    "# function is in turn passed as input to the last CustomLinearLayer.\n",
    "sequential_network = nn.Sequential(\n",
    "    CustomLinearLayer(num_in_features=5, num_out_features=2),\n",
    "    nn.ReLU(),\n",
    "    CustomLinearLayer(num_in_features=2, num_out_features=1)\n",
    ")\n",
    "print(sequential_network)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1., 2., 3., 4., 5.])\n",
      "torch.Size([5])\n"
     ]
    }
   ],
   "source": [
    "sample_input_2 = torch.tensor(data=[1, 2, 3, 4, 5], dtype=torch.float)\n",
    "print(sample_input_2)\n",
    "print(sample_input_2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-0.8641], grad_fn=<AddBackward0>)\n",
      "torch.Size([1])\n"
     ]
    }
   ],
   "source": [
    "output_2 = sequential_network(sample_input_2)\n",
    "print(output_2)\n",
    "print(output_2.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SubModules of a Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # These are the children or submodules of CustomNetwork since these define the layers of the\n",
    "        # CustomNetwork and are used in the forward method.\n",
    "        self.linear_layer0 = nn.Linear(in_features=3, out_features=4)\n",
    "        self.linear_layer1 = nn.Linear(in_features=4, out_features=1)\n",
    "    \n",
    "    def forward(self, input: Tensor) -> Tensor:\n",
    "        output = self.linear_layer0(input)\n",
    "        output = nn.ReLU(output)\n",
    "        output = self.linear_layer1(output)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CustomNetwork(\n",
      "  (linear_layer0): Linear(in_features=3, out_features=4, bias=True)\n",
      "  (linear_layer1): Linear(in_features=4, out_features=1, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "custom_network = CustomNetwork()\n",
    "print(custom_network)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('linear_layer0', Linear(in_features=3, out_features=4, bias=True))\n",
      "('linear_layer1', Linear(in_features=4, out_features=1, bias=True))\n"
     ]
    }
   ],
   "source": [
    "# The children or submodules of a module can be accessed via the 'named_children()' method.\n",
    "for child_module in custom_network.named_children():\n",
    "    print(child_module)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## State Dicionary of a module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The following documentation and examples have been copied from the official pytorch documentation.\n",
    "# (https://pytorch.org/docs/stable/notes/modules.html#module-state).\n",
    "#\n",
    "# A module's state_dict contains all the information necessary to hold a model. This information includes:\n",
    "# 1) Parameters: \n",
    "#       -- Trainable (learnable) model parameters.\n",
    "# 2) Buffers\n",
    "#       -- Non-Trainable (non-learnable) variables of the model.\n",
    "#       -- Variables that are not trainable by the model but affect the computations performed by the model.\n",
    "#       -- Buffers are of two types:\n",
    "#               -- Persistent Buffers\n",
    "#                       -- Contained within the state_dict.\n",
    "#               -- Non Persistent Buffers\n",
    "#                       -- Not contained within the state_dict.\n",
    "#  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# An example usage of Buffers is the running mean that is maintained when Batch Normalization is used \n",
    "# within the models. The mean is just computed repeatedly to be used during the inference time, but \n",
    "# it is not learnt (or trained) during model training.\n",
    "class RunningMean(nn.Module):\n",
    "  # Please ignore the meaning of momentum for now. We will deal with this in other notebooks.\n",
    "  def __init__(self, num_features, momentum=0.9):\n",
    "    super().__init__()\n",
    "    self.momentum = momentum\n",
    "    # registed_buffer is used to declare the parameters as buffers.\n",
    "    # https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module.register_buffer\n",
    "    # Excellent discussion (link below) on the usage of Buffers vs Parameters (with requires_grad = False)..\n",
    "    # https://discuss.pytorch.org/t/what-does-register-buffer-do/121091\n",
    "    # https://discuss.pytorch.org/t/what-is-the-difference-between-register-buffer-and-register-parameter-of-nn-module/32723\n",
    "    self.register_buffer(name='mean', tensor=torch.zeros(num_features), persistent=True)\n",
    "  def forward(self, x):\n",
    "    self.mean = self.momentum * self.mean + (1.0 - self.momentum) * x\n",
    "    return self.mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict([('mean', tensor([-0.1160,  0.1041,  0.1457, -0.1391]))])\n"
     ]
    }
   ],
   "source": [
    "# Notice that 'mean' tensor is saved in the state_dict since it is declared as 'persistent' buffer.\n",
    "rm = RunningMean(4)\n",
    "input = torch.randn(4)\n",
    "rm(input)\n",
    "print(rm.state_dict())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [nn.ModuleList](https://pytorch.org/docs/stable/generated/torch.nn.ModuleList.html#modulelist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ModuleList is useful when we want to dynamically create a list of modules. If we create submodules and add\n",
    "# them to a python list manually, then the submodules will not be registered as children of the parent module.\n",
    "# Using ModuleList to store the list of modules is useful because Pytorch automatically registers the modules\n",
    "# within the ModuleList as children of the parent module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DynamicLayer(nn.Module):\n",
    "    def __init__(self, num_layers: int):\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList([nn.Linear(4, 4) for _ in range(num_layers)])\n",
    "    def forward(self, input: Tensor) -> Tensor:\n",
    "        for layer in self.layers:\n",
    "        x = layer(x)\n",
    "        return x"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".pytorch_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
