{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In this notebook, you learn:\n",
    "#\n",
    "# 1) How to use KL divergence loss in pytorch?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "from torch import nn, Tensor\n",
    "from typing import Optional, Tuple"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [torch.nn.KLDivLoss](https://pytorch.org/docs/stable/generated/torch.nn.KLDivLoss.html#kldivloss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resources to learn about KL divergence:\n",
    "# \n",
    "# 1) https://www.countbayesie.com/blog/2017/5/9/kullback-leibler-divergence-explained\n",
    "#       -- Gives and intuitive explanation of KL Divergence\n",
    "# 2) https://encord.com/blog/kl-divergence-in-machine-learning/\n",
    "#       -- Similar to 1 but explains more in the context of machine learning.\n",
    "# 3) https://dibyaghosh.com/blog/probability/kldivergence.html\n",
    "#       -- Explains the math behind KL Divergence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some useful helper functions to assist while experimenting with KL Divergence loss.\n",
    "# \n",
    "# Just logs the tensor in a standard format. \n",
    "def LogInputTensor(input: Tensor, name: Optional[str]=None) -> None:\n",
    "    print(\"shape: \", input.shape)\n",
    "    print(f\"{name}: \\n\", input)\n",
    "    print(\"-\" * 150)\n",
    "\n",
    "# Generates a batch of input data. Output is 2D tensor of shape [batch_size, num_classes] within\n",
    "# the range [low, high).\n",
    "def generate_batch_of_input_data(batch_size: int, num_classes: int, low: Optional[float]=0.0, high: Optional[float]=1.0) -> Tensor:\n",
    "    return (torch.rand(size=(batch_size, num_classes), dtype=torch.float32) * (high - low)) + low\n",
    "\n",
    "# Applies log softmax to the input tensor.\n",
    "def apply_log_softmax(input: Tensor) -> Tensor:\n",
    "    # This is the natural log (i.e., base e) of the softmax function.\n",
    "    log_softmax = nn.LogSoftmax(dim=-1)\n",
    "    return log_softmax(input)\n",
    "\n",
    "# Applies softmax to the input tensor.\n",
    "def apply_softmax(input: Tensor) -> Tensor:\n",
    "    softmax = nn.Softmax(dim=-1)\n",
    "    return softmax(input)\n",
    "\n",
    "# Generates sample data to experiment with KL Divergence loss.\n",
    "# Returns (predictions, targets).\n",
    "def generate_sample_data(batch_size: int, num_classes: int) -> Tuple[Tensor, Tensor]:\n",
    "    predictions = generate_batch_of_input_data(batch_size, num_classes)\n",
    "    # nn.KLDivLoss expects the input to be log probabilities. \n",
    "    log_predictions = apply_log_softmax(predictions)\n",
    "    # Log the input tensor in the standard format.\n",
    "    LogInputTensor(input=log_predictions, name=\"log predictions\")\n",
    "    targets = generate_batch_of_input_data(batch_size, num_classes)\n",
    "    # Applying softmax to make sure that all the probabilities in the targets sum to 1.\n",
    "    targets = apply_softmax(targets)\n",
    "    # Log the input tensor in the standard format.\n",
    "    LogInputTensor(input=targets, name=\"targets\")\n",
    "    return log_predictions, targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KLDivLoss()\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "shape:  torch.Size([1, 5])\n",
      "log predictions: \n",
      " tensor([[-1.6134, -1.6146, -2.0372, -1.3749, -1.5208]])\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "shape:  torch.Size([1, 5])\n",
      "targets: \n",
      " tensor([[0.2921, 0.2377, 0.1326, 0.1551, 0.1825]])\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "shape:  torch.Size([1, 5])\n",
      "loss_output_1: \n",
      " tensor([[ 0.1117,  0.0423,  0.0022, -0.0758, -0.0329]])\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "calculated KL divergence contribution:  0.11173786103748762   loss_output_1[0][0]:  0.11173787713050842\n",
      "calculated KL divergence contribution:  0.042294728042901894   loss_output_1[0][1]:  0.04229474067687988\n",
      "calculated KL divergence contribution:  0.0021778623088963447   loss_output_1[0][2]:  0.0021778643131256104\n",
      "calculated KL divergence contribution:  -0.07579706657902392   loss_output_1[0][3]:  -0.07579706609249115\n",
      "calculated KL divergence contribution:  -0.032871048807069495   loss_output_1[0][4]:  -0.0328710675239563\n"
     ]
    }
   ],
   "source": [
    "# When reduction is set to \"none\" (actually anything other than \"mean\", \"sum\" or \"batchmean\"), the loss is not reduced.\n",
    "# That means, for each point in the probability distribution, the contribution towards the KL divergence is calculated\n",
    "# and this is returned directly without summing up the contributions which gives the actual KL divergence value by \n",
    "# mathematical definition.\n",
    "# \n",
    "# The mathematical formula for KL divergence is --> KL(P || Q) = sum(P(x) * log(P(x) / Q(x)))\n",
    "# where P and Q are probability distributions.\n",
    "#\n",
    "# In our example below, we are calculating KL divergence between two probability distributions, targets (P) and \n",
    "# predictions (Q). Lets say targets = [p1, p2, p3, p4, p5] and predictions = [log(q1), log(q2), log(q3), log(q4), log(q5)]. \n",
    "# Here log is the natural logarithm. It's doesn't really matter whether we use natual logarithm or logarithm with any other \n",
    "# base since the base of the logarithm cancels out (we calculate log(p/q)) in the KL divergence formula.\n",
    "# Then the nn.KLDivLoss now calculates the contribution of each point in (P, Q) towards the KL divergence and returns it.\n",
    "# So, the output will be [p1 * log(p1 / q1), p2 * log(p2 / q2), p3 * log(p3 / q3), p4 * log(p4 / q4), p5 * log(p5 / q5)]\n",
    "kl_loss_1 = nn.KLDivLoss(reduction=\"none\")\n",
    "print(kl_loss_1)\n",
    "print(\"-\" * 150)\n",
    "log_predictions_1, targets_1 = generate_sample_data(batch_size=1, num_classes=5)\n",
    "loss_output_1 = kl_loss_1(log_predictions_1, targets_1)\n",
    "LogInputTensor(input=loss_output_1, name=\"loss_output_1\")\n",
    "# Lets show that KL divergence contributions calculated by nn.KLDivLoss are same as the ones calculated manually.\n",
    "for i in range(5):\n",
    "    # Calculating the contribution of each point in (P, Q) towards the KL divergence.\n",
    "    kl_divergence_contribution_1 = targets_1[0][i].item() * (math.log(targets_1[0][i].item()) - log_predictions_1[0][i].item())\n",
    "    # Both (explicitly calculated value, output of pytorch library) the values should be the same.\n",
    "    print(\"calculated KL divergence contribution: \", kl_divergence_contribution_1, \" \", f\"loss_output_1[0][{i}]: \", loss_output_1[0][i].item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KLDivLoss()\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "shape:  torch.Size([1, 5])\n",
      "log predictions: \n",
      " tensor([[-1.7925, -1.1957, -2.0741, -1.8951, -1.3666]])\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "shape:  torch.Size([1, 5])\n",
      "targets: \n",
      " tensor([[0.1440, 0.1824, 0.2032, 0.1891, 0.2813]])\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "shape:  torch.Size([])\n",
      "loss_output_2: \n",
      " tensor(0.0555)\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "calculated KL divergence:  0.05545330548664444   loss_output_2:  0.055453330278396606\n"
     ]
    }
   ],
   "source": [
    "# When reduction is set to \"sum\", the contribution from each class is summed up and the final value is returned.\n",
    "# This is the actual KL divergence value (by mathematical definition) between the two probability distributions.\n",
    "kl_loss_2 = nn.KLDivLoss(reduction=\"sum\")\n",
    "print(kl_loss_2)\n",
    "print(\"-\" * 150)\n",
    "log_predictions_2, targets_2 = generate_sample_data(batch_size=1, num_classes=5)\n",
    "loss_output_2 = kl_loss_2(log_predictions_2, targets_2)\n",
    "LogInputTensor(input=loss_output_2, name=\"loss_output_2\")\n",
    "kl_divergence_2 = 0.0\n",
    "# Lets show that KL divergence calculated by nn.KLDivLoss is the same as the value calculated manually.\n",
    "for i in range(5):\n",
    "    # kl_divergence is just the sum of all the contributions from each class.\n",
    "    kl_divergence_2 += targets_2[0][i].item() * (math.log(targets_2[0][i].item()) - log_predictions_2[0][i].item())\n",
    "# Both (explicitly calculated value, output of pytorch library) the values should be the same.\n",
    "print(\"calculated KL divergence: \", kl_divergence_2, \" \", f\"loss_output_2: \", loss_output_2.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KLDivLoss()\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "shape:  torch.Size([1, 5])\n",
      "log predictions: \n",
      " tensor([[-1.4585, -1.5000, -1.4662, -1.7847, -1.9265]])\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "shape:  torch.Size([1, 5])\n",
      "targets: \n",
      " tensor([[0.1151, 0.1317, 0.2311, 0.2611, 0.2611]])\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "shape:  torch.Size([])\n",
      "loss_output_3: \n",
      " tensor(0.1175)\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "calculated KL divergence:  0.11753872195015953   loss_output_3:  0.11753872036933899\n"
     ]
    }
   ],
   "source": [
    "# When reduction is set to \"batchmean\", the KL Divergence from each example is summed up and then divided by the batch size.\n",
    "kl_loss_3 = nn.KLDivLoss(reduction=\"batchmean\")\n",
    "print(kl_loss_3)\n",
    "print(\"-\" * 150)\n",
    "log_predictions_3, targets_3 = generate_sample_data(batch_size=1, num_classes=5)\n",
    "loss_output_3 = kl_loss_3(log_predictions_3, targets_3)\n",
    "LogInputTensor(input=loss_output_3, name=\"loss_output_3\")\n",
    "kl_divergence_contribution_3 = 0.0\n",
    "# Lets show that KL divergence calculated by nn.KLDivLoss is the same as the value calculated manually.\n",
    "for i in range(5):\n",
    "    kl_divergence_contribution_3 += targets_3[0][i].item() * (math.log(targets_3[0][i].item()) - log_predictions_3[0][i].item())\n",
    "# Both (explicitly calculated value, output of pytorch library) the values should be the same. In this case, since we just \n",
    "# have a single example (batch_size=1), the batch_mean value should be the same as the KL divergence value for the calculated \n",
    "# sentence. It is the same as the KL divergence value when reduction is set to \"sum\" since batch_size is 1.\n",
    "kl_divergence_batch_mean_3 = kl_divergence_contribution_3 / 1\n",
    "print(\"calculated KL divergence: \", kl_divergence_batch_mean_3, \" \", f\"loss_output_3: \", loss_output_3.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KLDivLoss()\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "shape:  torch.Size([2, 5])\n",
      "log predictions: \n",
      " tensor([[-1.4476, -1.5270, -1.9493, -2.1356, -1.2478],\n",
      "        [-1.9801, -1.3222, -1.4569, -2.0759, -1.4398]])\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "shape:  torch.Size([2, 5])\n",
      "targets: \n",
      " tensor([[0.1750, 0.1804, 0.1893, 0.1756, 0.2796],\n",
      "        [0.2650, 0.1472, 0.1578, 0.2739, 0.1562]])\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "shape:  torch.Size([])\n",
      "loss_output_4: \n",
      " tensor(0.1018)\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "calculated KL divergence:  0.10180229888377959   loss_output_4:  0.101802296936512\n"
     ]
    }
   ],
   "source": [
    "# When reduction is set to \"batchmean\", the KL Divergence from each example is summed up and then divided by the batch size.\n",
    "# Lets set the batch_size to 2 and see how the KL divergence is calculated.\n",
    "kl_loss_4 = nn.KLDivLoss(reduction=\"batchmean\")\n",
    "print(kl_loss_4)\n",
    "print(\"-\" * 150)\n",
    "log_predictions_4, targets_4 = generate_sample_data(batch_size=2, num_classes=5)\n",
    "loss_output_4 = kl_loss_4(log_predictions_4, targets_4)\n",
    "LogInputTensor(input=loss_output_4, name=\"loss_output_4\")\n",
    "kl_divergence_batch_mean_4 = 0.0\n",
    "# Iterates over examples. Example corresponds to one (input, output) pair in the batch.\n",
    "for ex_idx in range(2):\n",
    "    kl_divergence_for_example_4 = 0.0\n",
    "    # Iterates over classes within an example.\n",
    "    for prob_idx in range(5):\n",
    "        kl_divergence_for_example_4 += targets_4[ex_idx][prob_idx].item() * (math.log(targets_4[ex_idx][prob_idx].item()) - log_predictions_4[ex_idx][prob_idx].item())\n",
    "    kl_divergence_batch_mean_4 += kl_divergence_for_example_4\n",
    "# We have two examples in the batch. So, we divide the sum by 2.\n",
    "kl_divergence_batch_mean_4 /= 2\n",
    "# Both (explicitly calculated value, output of pytorch library) the values should be the same.\n",
    "print(\"calculated KL divergence: \", kl_divergence_batch_mean_4, \" \", f\"loss_output_4: \", loss_output_4.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KLDivLoss()\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "shape:  torch.Size([1, 5])\n",
      "predictions_4_5: \n",
      " tensor([[-1.7047, -1.9487, -1.6372, -1.9514, -1.0815]])\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "shape:  torch.Size([1, 5])\n",
      "targets_4_5: \n",
      " tensor([[0., 0., 0., 0., 1.]])\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "shape:  torch.Size([1, 5])\n",
      "kl_loss_output_4_5: \n",
      " tensor([[0.0000, 0.0000, 0.0000, 0.0000, 1.0815]])\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Lets try to see what happens when the probability of a target class is 0. The contributions from the \n",
    "# classes where the target probability is 0 should be 0. Lets see if this is the case.\n",
    "kl_div_loss_4_5 = nn.KLDivLoss(reduction=\"none\")\n",
    "print(kl_div_loss_4_5)\n",
    "print(\"-\" * 150)\n",
    "predictions_4_5 = torch.rand(size=(1, 5), dtype=torch.float32)\n",
    "log_predictions_4_5 = apply_log_softmax(predictions_4_5)\n",
    "LogInputTensor(input=log_predictions_4_5, name=\"predictions_4_5\")\n",
    "targets_4_5 = torch.tensor([[0.0, 0.0, 0.0, 0.0, 1.0]], dtype=torch.float32)\n",
    "LogInputTensor(input=targets_4_5, name=\"targets_4_5\")\n",
    "kl_loss_output_4_5 = kl_div_loss_4_5(log_predictions_4_5, targets_4_5)\n",
    "LogInputTensor(input=kl_loss_output_4_5, name=\"kl_loss_output_4_5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding nn.KLDivLoss when the input and target are 3D tensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The transformers usually deal with 3D tensors of shape [batch_size, seq_len, num_classes]. Each sequence in the batch\n",
    "# contains multiple tokens (seq_len) and each token is represented as a probability distribution over num_classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_3D_batch_of_input_data(batch_size: int, seq_len: int, vocab_size: int, low: Optional[float]=0.0, high: Optional[float]=1.0) -> Tensor:\n",
    "    return (torch.rand(size=(batch_size, seq_len, vocab_size), dtype=torch.float32) * (high - low)) + low\n",
    "\n",
    "# Generates sample data to experiment with KL Divergence loss.\n",
    "# Returns (predictions, targets).\n",
    "def generate_3D_sample_data(batch_size: int, seq_len: int, vocab_size: int) -> Tuple[Tensor, Tensor]:\n",
    "    predictions = generate_3D_batch_of_input_data(batch_size, seq_len, vocab_size)\n",
    "    log_predictions = apply_log_softmax(predictions)\n",
    "    LogInputTensor(input=log_predictions, name=\"log predictions\")\n",
    "    targets = generate_3D_batch_of_input_data(batch_size, seq_len, vocab_size)\n",
    "    targets = apply_softmax(targets)\n",
    "    LogInputTensor(input=targets, name=\"targets\")\n",
    "    return log_predictions, targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of examples (sentences) in the input batch.\n",
    "batch_size = 2\n",
    "# Number of tokens in each sentence.\n",
    "seq_len = 2\n",
    "# Number of classes (vocabulary size).\n",
    "num_classes = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape:  torch.Size([2, 2, 5])\n",
      "log predictions: \n",
      " tensor([[[-1.8828, -1.8708, -1.4231, -1.2687, -1.7623],\n",
      "         [-1.9382, -1.3630, -1.9212, -1.7678, -1.2622]],\n",
      "\n",
      "        [[-1.5502, -1.7241, -1.3850, -1.7437, -1.6914],\n",
      "         [-1.6912, -1.6190, -1.4458, -1.6422, -1.6686]]])\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "shape:  torch.Size([2, 2, 5])\n",
      "targets: \n",
      " tensor([[[0.2483, 0.2509, 0.2331, 0.1465, 0.1211],\n",
      "         [0.1610, 0.2124, 0.1847, 0.3007, 0.1412]],\n",
      "\n",
      "        [[0.2326, 0.1918, 0.2423, 0.1271, 0.2061],\n",
      "         [0.2354, 0.1480, 0.1996, 0.2189, 0.1981]]])\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "shape:  torch.Size([2, 2, 5])\n",
      "loss_output_5: \n",
      " tensor([[[ 0.1217,  0.1225, -0.0077, -0.0955, -0.0423],\n",
      "         [ 0.0180, -0.0395,  0.0428,  0.1703, -0.0982]],\n",
      "\n",
      "        [[ 0.0214,  0.0139, -0.0079, -0.0405,  0.0231],\n",
      "         [ 0.0577, -0.0432, -0.0331,  0.0270,  0.0099]]])\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "calculated KL divergence:  0.12165624785962328   loss_output_5[0][0][0]:  0.12165623903274536\n",
      "calculated KL divergence:  0.12252060807858856   loss_output_5[0][0][1]:  0.12252059578895569\n",
      "calculated KL divergence:  -0.0077252263277098054   loss_output_5[0][0][2]:  -0.007725238800048828\n",
      "calculated KL divergence:  -0.09551534986099695   loss_output_5[0][0][3]:  -0.09551535546779633\n",
      "calculated KL divergence:  -0.04227374779311401   loss_output_5[0][0][4]:  -0.04227373003959656\n",
      "calculated KL divergence:  0.017975224545571326   loss_output_5[0][1][0]:  0.017975211143493652\n",
      "calculated KL divergence:  -0.0395298979141613   loss_output_5[0][1][1]:  -0.03952991962432861\n",
      "calculated KL divergence:  0.04283944389901774   loss_output_5[0][1][2]:  0.0428394079208374\n",
      "calculated KL divergence:  0.170309084251473   loss_output_5[0][1][3]:  0.17030909657478333\n",
      "calculated KL divergence:  -0.09818625095208901   loss_output_5[0][1][4]:  -0.09818625450134277\n",
      "calculated KL divergence:  0.0213845099760625   loss_output_5[1][0][0]:  0.021384507417678833\n",
      "calculated KL divergence:  0.01394788823782368   loss_output_5[1][0][1]:  0.013947904109954834\n",
      "calculated KL divergence:  -0.007873772705323644   loss_output_5[1][0][2]:  -0.007873773574829102\n",
      "calculated KL divergence:  -0.04053595168350091   loss_output_5[1][0][3]:  -0.040535956621170044\n",
      "calculated KL divergence:  0.023111550521976453   loss_output_5[1][0][4]:  0.023111552000045776\n",
      "calculated KL divergence:  0.05766733391056198   loss_output_5[1][1][0]:  0.05766734480857849\n",
      "calculated KL divergence:  -0.04318042297631154   loss_output_5[1][1][1]:  -0.0431804358959198\n",
      "calculated KL divergence:  -0.03309320805932437   loss_output_5[1][1][2]:  -0.03309318423271179\n",
      "calculated KL divergence:  0.026953373237817945   loss_output_5[1][1][3]:  0.02695336937904358\n",
      "calculated KL divergence:  0.009870826408750344   loss_output_5[1][1][4]:  0.009870797395706177\n"
     ]
    }
   ],
   "source": [
    "# Lets calculate KL Divergence using the nn.KLDivLoss with reduction set to \"none\".\n",
    "log_predictions_5, targets_5 = generate_3D_sample_data(batch_size, seq_len, num_classes)\n",
    "kl_div_loss_5 = nn.KLDivLoss(reduction='none')\n",
    "loss_output_5 = kl_div_loss_5(log_predictions_5, targets_5)\n",
    "LogInputTensor(input=loss_output_5, name=\"loss_output_5\")\n",
    "# Now lets try to calculate the KL divergence contribution for each point in the probability distribution and compare\n",
    "# it with the outputs returned by the pytorch library.\n",
    "for seq_idx in range(batch_size):\n",
    "    for token_idx in range(seq_len):\n",
    "        for class_idx in range(num_classes):\n",
    "            kl_divergence_contribution_5 = targets_5[seq_idx][token_idx][class_idx].item() * (math.log(targets_5[seq_idx][token_idx][class_idx].item()) - log_predictions_5[seq_idx][token_idx][class_idx].item())\n",
    "            # Both the values should be the same as explained in the 2D case.\n",
    "            print(\"calculated KL divergence: \", kl_divergence_contribution_5, \" \", f\"loss_output_5[{seq_idx}][{token_idx}][{class_idx}]: \", loss_output_5[seq_idx][token_idx][class_idx].item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape:  torch.Size([2, 2, 5])\n",
      "log predictions: \n",
      " tensor([[[-1.1395, -1.9188, -1.8255, -1.5439, -1.8415],\n",
      "         [-2.0697, -1.4438, -2.0117, -1.5855, -1.2068]],\n",
      "\n",
      "        [[-1.5631, -1.6739, -1.4507, -1.6425, -1.7425],\n",
      "         [-1.4436, -1.5906, -1.8617, -1.9599, -1.3324]]])\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "shape:  torch.Size([2, 2, 5])\n",
      "targets: \n",
      " tensor([[[0.1868, 0.1228, 0.1924, 0.2534, 0.2446],\n",
      "         [0.2133, 0.1050, 0.2529, 0.2306, 0.1983]],\n",
      "\n",
      "        [[0.2136, 0.1437, 0.2141, 0.2467, 0.1819],\n",
      "         [0.2857, 0.2436, 0.1803, 0.1605, 0.1299]]])\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "shape:  torch.Size([])\n",
      "loss_output_6: \n",
      " tensor(0.1308)\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "calculated KL divergence:  0.13080740392440268   loss_output_6:  0.13080735504627228\n"
     ]
    }
   ],
   "source": [
    "# Lets calculate KL Divergence using the nn.KLDivLoss with reduction set to \"batchmean\".\n",
    "log_predictions_6, targets_6 = generate_3D_sample_data(batch_size, seq_len, num_classes)\n",
    "kl_div_loss_6 = nn.KLDivLoss(reduction='batchmean')\n",
    "loss_output_6 = kl_div_loss_6(log_predictions_6, targets_6)\n",
    "LogInputTensor(input=loss_output_6, name=\"loss_output_6\")\n",
    "kl_divergence_batch_mean_6 = 0.0\n",
    "for seq_idx in range(batch_size):\n",
    "    kl_divergence_for_sentence_6 = 0.0\n",
    "    for token_idx in range(seq_len):\n",
    "        for class_idx in range(num_classes):\n",
    "            kl_divergence_for_sentence_6 += targets_6[seq_idx][token_idx][class_idx].item() * (math.log(targets_6[seq_idx][token_idx][class_idx].item()) - log_predictions_6[seq_idx][token_idx][class_idx].item())\n",
    "    kl_divergence_batch_mean_6 += kl_divergence_for_sentence_6\n",
    "# Take the mean of the KL divergence for each sentence in the batch.\n",
    "kl_divergence_batch_mean_6 /= batch_size\n",
    "# Both the values should be the same.\n",
    "print(\"calculated KL divergence: \", kl_divergence_batch_mean_6, \" \", f\"loss_output_6: \", loss_output_6.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape:  torch.Size([2, 2, 5])\n",
      "log predictions: \n",
      " tensor([[[-1.3373, -1.3475, -2.0353, -1.7831, -1.7215],\n",
      "         [-1.6993, -1.3270, -1.4256, -1.9131, -1.8083]],\n",
      "\n",
      "        [[-1.7127, -1.3311, -1.5016, -1.7193, -1.8744],\n",
      "         [-1.8037, -1.5605, -1.4432, -1.9530, -1.3973]]])\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "shape:  torch.Size([2, 2, 5])\n",
      "targets: \n",
      " tensor([[[0.1858, 0.1909, 0.1305, 0.1875, 0.3053],\n",
      "         [0.2553, 0.1131, 0.1268, 0.2315, 0.2734]],\n",
      "\n",
      "        [[0.2598, 0.1946, 0.2566, 0.1539, 0.1350],\n",
      "         [0.1166, 0.2467, 0.1591, 0.1983, 0.2792]]])\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "shape:  torch.Size([])\n",
      "loss_output_7: \n",
      " tensor(0.2802)\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "calculated KL divergence:  0.28019537932624977   loss_output_7:  0.28019535541534424\n"
     ]
    }
   ],
   "source": [
    "# Lets calculate KL Divergence using the nn.KLDivLoss with reduction set to \"sum\".\n",
    "log_predictions_7, targets_7 = generate_3D_sample_data(batch_size, seq_len, num_classes)\n",
    "kl_div_loss_7 = nn.KLDivLoss(reduction='sum')\n",
    "loss_output_7 = kl_div_loss_7(log_predictions_7, targets_7)\n",
    "LogInputTensor(input=loss_output_7, name=\"loss_output_7\")\n",
    "kl_divergence_sum_7 = 0.0\n",
    "for seq_idx in range(batch_size):\n",
    "    for token_idx in range(seq_len):\n",
    "        for class_idx in range(num_classes):\n",
    "            kl_divergence_sum_7 += targets_7[seq_idx][token_idx][class_idx].item() * (math.log(targets_7[seq_idx][token_idx][class_idx].item()) - log_predictions_7[seq_idx][token_idx][class_idx].item())\n",
    "# Both the values should be the same.\n",
    "print(\"calculated KL divergence: \", kl_divergence_sum_7, \" \", f\"loss_output_7: \", loss_output_7.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".pytorch_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
