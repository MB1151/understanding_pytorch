{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In this notebook, you learn:\n",
    "# \n",
    "# 1) How to use Adam optimization in Pytorch?\n",
    "# \n",
    "# Resources to go through before continuing with this notebook:\n",
    "# 1) https://www.youtube.com/watch?v=lAq96T8FkTw&list=PLkDaE6sCZn6Hn0vK8co82zjQtt3T2Nkqc&index=18\n",
    "#       -- Part 1 of the videos on exponential weighted averages.\n",
    "#       -- Explains how exponential weighted averages work is calculated.\n",
    "# 2) https://www.youtube.com/watch?v=NxTFlzBjS-4&list=PLkDaE6sCZn6Hn0vK8co82zjQtt3T2Nkqc&index=19\n",
    "#       -- Part 2 of the videos on exponential weighted averages.\n",
    "#       -- Explains why exponential weighted averages work.\n",
    "# 3) https://www.youtube.com/watch?v=lWzo8CajF5s&list=PLkDaE6sCZn6Hn0vK8co82zjQtt3T2Nkqc&index=19\n",
    "#       -- Part 3 of the videos on exponential weighted averages.\n",
    "#       -- Explains bias correction in exponential weighted averages.\n",
    "# 4) https://www.youtube.com/watch?v=k8fTYJPd3_I&list=PLkDaE6sCZn6Hn0vK8co82zjQtt3T2Nkqc&index=20\n",
    "#       -- Explains Gradient Descent with Momentum.\n",
    "# 5) https://www.youtube.com/watch?v=_e-LFe_igno&list=PLkDaE6sCZn6Hn0vK8co82zjQtt3T2Nkqc&index=22\n",
    "#       -- Explains RMS prop.\n",
    "# 6) https://www.youtube.com/watch?v=JXQT_vxqwIs&list=PLkDaE6sCZn6Hn0vK8co82zjQtt3T2Nkqc&index=22\n",
    "#       -- Explains Adam optimization algorithm.\n",
    "# 7) https://www.linkedin.com/pulse/getting-know-adam-optimization-comprehensive-guide-kiran-kumar/\n",
    "#       -- Reiterates the concepts of Adam optimization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimizers are used to update the weights of the neural network in order to minimize the loss function. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [torch.optim.Adam](https://pytorch.org/docs/stable/generated/torch.optim.Adam.html#torch.optim.Adam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants used in the notebook.\n",
    "# Number of input features for our neural network (linear layer).\n",
    "in_features = 4\n",
    "# Number of output features for our neural network (linear layer).\n",
    "out_features = 6\n",
    "# Number of samples in our dataset.\n",
    "num_samples = 2\n",
    "# Number of iterations to train the neural network.\n",
    "num_iterations = 2\n",
    "# Learning rate for the optimizer.\n",
    "learning_rate = 0.01\n",
    "# Hyperparameter to calculate the m1 moment in the optimizer. This roughly corresponds to averaging over the\n",
    "# last 10 (1/(1-beta_1)) sets of gradients. This comes from 'Gradient Descent with Momentum' algorithm.\n",
    "beta_1 = 0.9\n",
    "# Hyperparameter to calculate the m1 moment in the optimizer. This roughly corresponds to averaging over the\n",
    "# last 1000 (1/(1-beta_2)) sets of gradients. This comes from 'RMS prop' algorithm.\n",
    "beta_2 = 0.999\n",
    "# Small value to avoid division by zero in the optimizer.\n",
    "epsilon = 1e-8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear(in_features=4, out_features=6, bias=True)\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "<generator object Module.parameters at 0x7ffa60245000>\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "parameters:  weight  -  torch.Size([6, 4])  -  tensor([[ 0.1006,  0.2277, -0.2466, -0.0159],\n",
      "        [-0.2504,  0.1124,  0.4456,  0.3830],\n",
      "        [-0.3520, -0.0375,  0.1437,  0.2367],\n",
      "        [-0.0943, -0.1823, -0.3903, -0.1696],\n",
      "        [ 0.3875,  0.3849,  0.3608,  0.2921],\n",
      "        [-0.3984, -0.4341,  0.1094, -0.2876]])\n",
      "gradients_of_parameters:  weight  -  no shape to print   -  None\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "parameters:  bias  -  torch.Size([6])  -  tensor([-0.3227,  0.4481, -0.0626,  0.0035, -0.4698,  0.1999])\n",
      "gradients_of_parameters:  bias  -  no shape to print   -  None\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Creating a linear layer which will serve as the neural network for this experiment.\n",
    "linear_layer = nn.Linear(in_features=in_features, out_features=out_features, bias=True)\n",
    "print(linear_layer)\n",
    "print(\"-\" * 150)\n",
    "# This is what we pass to the optimizer to update the weights of the neural network.\n",
    "weights_or_params = linear_layer.parameters()\n",
    "print(weights_or_params)\n",
    "print(\"-\" * 150)\n",
    "# Peeking into the actual parameters of the linear layer.\n",
    "for name, param in linear_layer.named_parameters():\n",
    "  print(\"parameters: \", name, \" - \", param.data.shape, \" - \", param.data)\n",
    "  # Since we have not yet called the backward function, the gradients are not yet calculated. So, this will print None.\n",
    "  print(\"gradients_of_parameters: \", name, \" - \", param.grad.shape if param.grad else \"no shape to print \", \" - \", param.grad)\n",
    "  print(\"-\" * 150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape:  torch.Size([2, 4])\n",
      "input_data: \n",
      " tensor([[ 0.0391,  0.2567, -1.8606,  0.2611],\n",
      "        [ 1.2128, -0.2215, -1.3360, -0.4727]])\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "shape:  torch.Size([2, 6])\n",
      "output_data: \n",
      " tensor([[ 0.5449, -0.4881, -0.3103, -1.7100,  0.1560, -0.0139],\n",
      "        [ 0.9330,  0.0412, -1.0586,  0.5111,  1.5341,  0.0960]])\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Create random input and output tensors for the neural network.\n",
    "input_data = torch.randn(size=(num_samples, in_features), dtype=torch.float32)\n",
    "print(\"shape: \", input_data.shape)\n",
    "print(\"input_data: \\n\", input_data)\n",
    "print(\"-\" * 150)\n",
    "output_data = torch.randn(size=(num_samples, out_features), dtype=torch.float32)  \n",
    "print(\"shape: \", output_data.shape)\n",
    "print(\"output_data: \\n\", output_data)\n",
    "print(\"-\" * 150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSELoss()\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "Adam (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    capturable: False\n",
      "    differentiable: False\n",
      "    eps: 1e-08\n",
      "    foreach: None\n",
      "    fused: None\n",
      "    lr: 0.01\n",
      "    maximize: False\n",
      "    weight_decay: 0\n",
      ")\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "{'state': {}, 'param_groups': [{'lr': 0.01, 'betas': (0.9, 0.999), 'eps': 1e-08, 'weight_decay': 0, 'amsgrad': False, 'maximize': False, 'foreach': None, 'capturable': False, 'differentiable': False, 'fused': None, 'params': [0, 1]}]}\n"
     ]
    }
   ],
   "source": [
    "# Loss function to calculate the loss between the predicted output and the actual output. This is just a \n",
    "# simple mean squared error loss function.\n",
    "loss_fn = nn.MSELoss()\n",
    "print(loss_fn)\n",
    "print(\"-\" * 150)\n",
    "adam_optimizer = torch.optim.Adam(params=linear_layer.parameters(), lr=learning_rate, betas=(beta_1, beta_2), eps=epsilon)\n",
    "print(adam_optimizer)\n",
    "print(\"-\" * 150)\n",
    "# Before training the netural network, there are no gradients calculated. So, the state corresponding to the moving \n",
    "# averages of the gradients and the squared moving averages of the gradients are not stored in the state_dict.\n",
    "print(adam_optimizer.state_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'state': {0: {'step': tensor(1.), 'exp_avg': tensor([[-0.0174,  0.0016,  0.0297,  0.0051],\n",
      "        [-0.0140,  0.0035,  0.0085,  0.0065],\n",
      "        [ 0.0055, -0.0009, -0.0067, -0.0021],\n",
      "        [ 0.0019,  0.0100, -0.0732,  0.0100],\n",
      "        [-0.0460,  0.0035,  0.0842,  0.0128],\n",
      "        [-0.0061,  0.0003,  0.0125,  0.0015]]), 'exp_avg_sq': tensor([[3.0114e-05, 2.6479e-07, 8.8434e-05, 2.6514e-06],\n",
      "        [1.9494e-05, 1.2561e-06, 7.2729e-06, 4.2039e-06],\n",
      "        [3.0697e-06, 8.6436e-08, 4.4467e-06, 4.2999e-07],\n",
      "        [3.7330e-07, 9.9159e-06, 5.3531e-04, 1.0093e-05],\n",
      "        [2.1145e-04, 1.2465e-06, 7.0866e-04, 1.6451e-05],\n",
      "        [3.6670e-06, 6.8866e-09, 1.5578e-05, 2.1814e-07]])}, 1: {'step': tensor(1.), 'exp_avg': tensor([-0.0200, -0.0079,  0.0049,  0.0394, -0.0558, -0.0081]), 'exp_avg_sq': tensor([3.9857e-05, 6.1839e-06, 2.3710e-06, 1.5537e-04, 3.1100e-04, 6.5399e-06])}}, 'param_groups': [{'lr': 0.01, 'betas': (0.9, 0.999), 'eps': 1e-08, 'weight_decay': 0, 'amsgrad': False, 'maximize': False, 'foreach': None, 'capturable': False, 'differentiable': False, 'fused': None, 'params': [0, 1]}]}\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "shape:  torch.Size([6, 4])  name:  weight\n",
      "param.data:  tensor([[ 0.1106,  0.2177, -0.2566, -0.0259],\n",
      "        [-0.2404,  0.1024,  0.4356,  0.3730],\n",
      "        [-0.3620, -0.0275,  0.1537,  0.2467],\n",
      "        [-0.1043, -0.1923, -0.3803, -0.1796],\n",
      "        [ 0.3975,  0.3749,  0.3508,  0.2821],\n",
      "        [-0.3884, -0.4441,  0.0994, -0.2976]])\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "shape:  torch.Size([6, 4])  name:  weight\n",
      "param.grad:  tensor([[-0.1735,  0.0163,  0.2974,  0.0515],\n",
      "        [-0.1396,  0.0354,  0.0853,  0.0648],\n",
      "        [ 0.0554, -0.0093, -0.0667, -0.0207],\n",
      "        [ 0.0193,  0.0996, -0.7317,  0.1005],\n",
      "        [-0.4598,  0.0353,  0.8418,  0.1283],\n",
      "        [-0.0606,  0.0026,  0.1248,  0.0148]])\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "shape:  torch.Size([6])  name:  bias\n",
      "param.data:  tensor([-0.3127,  0.4581, -0.0726, -0.0065, -0.4598,  0.2099])\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "shape:  torch.Size([6])  name:  bias\n",
      "param.grad:  tensor([-0.1996, -0.0786,  0.0487,  0.3942, -0.5577, -0.0809])\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "{'state': {0: {'step': tensor(2.), 'exp_avg': tensor([[-0.0321,  0.0030,  0.0548,  0.0096],\n",
      "        [-0.0257,  0.0067,  0.0145,  0.0121],\n",
      "        [ 0.0097, -0.0017, -0.0110, -0.0037],\n",
      "        [ 0.0031,  0.0189, -0.1373,  0.0192],\n",
      "        [-0.0865,  0.0067,  0.1583,  0.0241],\n",
      "        [-0.0106,  0.0004,  0.0220,  0.0026]]), 'exp_avg_sq': tensor([[5.7245e-05, 5.1180e-07, 1.6705e-04, 5.0680e-06],\n",
      "        [3.6607e-05, 2.4725e-06, 1.1951e-05, 8.1099e-06],\n",
      "        [5.2450e-06, 1.6291e-07, 6.9273e-06, 7.6921e-07],\n",
      "        [5.5057e-07, 1.9741e-05, 1.0456e-03, 2.0329e-05],\n",
      "        [4.1472e-04, 2.4533e-06, 1.3885e-03, 3.2298e-05],\n",
      "        [6.3490e-06, 1.1194e-08, 2.7222e-05, 3.7325e-07]])}, 1: {'step': tensor(2.), 'exp_avg': tensor([-0.0368, -0.0138,  0.0081,  0.0738, -0.1049, -0.0143]), 'exp_avg_sq': tensor([7.5388e-05, 1.0747e-05, 3.7862e-06, 3.0246e-04, 6.0949e-04, 1.1409e-05])}}, 'param_groups': [{'lr': 0.01, 'betas': (0.9, 0.999), 'eps': 1e-08, 'weight_decay': 0, 'amsgrad': False, 'maximize': False, 'foreach': None, 'capturable': False, 'differentiable': False, 'fused': None, 'params': [0, 1]}]}\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "shape:  torch.Size([6, 4])  name:  weight\n",
      "param.data:  tensor([[ 0.1206,  0.2077, -0.2666, -0.0359],\n",
      "        [-0.2304,  0.0924,  0.4257,  0.3630],\n",
      "        [-0.3719, -0.0175,  0.1636,  0.2566],\n",
      "        [-0.1141, -0.2023, -0.3703, -0.1896],\n",
      "        [ 0.4075,  0.3649,  0.3408,  0.2721],\n",
      "        [-0.3785, -0.4539,  0.0895, -0.3075]])\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "shape:  torch.Size([6, 4])  name:  weight\n",
      "param.grad:  tensor([[-0.1648,  0.0157,  0.2805,  0.0492],\n",
      "        [-0.1309,  0.0349,  0.0684,  0.0625],\n",
      "        [ 0.0467, -0.0087, -0.0498, -0.0184],\n",
      "        [ 0.0133,  0.0992, -0.7147,  0.1012],\n",
      "        [-0.4511,  0.0348,  0.8250,  0.1260],\n",
      "        [-0.0518,  0.0021,  0.1080,  0.0125]])\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "shape:  torch.Size([6])  name:  bias\n",
      "param.data:  tensor([-0.3028,  0.4681, -0.0825, -0.0165, -0.4498,  0.2199])\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "shape:  torch.Size([6])  name:  bias\n",
      "param.grad:  tensor([-0.1886, -0.0676,  0.0377,  0.3837, -0.5466, -0.0698])\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# For now lets assume the entire dataset is processed at once and there are no mini-batches.\n",
    "for epoch in range(num_iterations):\n",
    "    # Zero the gradients of the parameters of the neural network.\n",
    "    adam_optimizer.zero_grad()\n",
    "    # Forward pass. Find the output predictions of the neural network.\n",
    "    predictions = linear_layer(input_data)\n",
    "    # Calculate the loss.\n",
    "    loss = loss_fn(predictions, output_data)\n",
    "    # Backward pass. Calculate the gradients of the loss with respect to the parameters of the neural network.\n",
    "    loss.backward()\n",
    "    # Update the weights of the neural network using the optimizer.\n",
    "    adam_optimizer.step()\n",
    "    # Print the state of the adam_optimizer. After each step, the state_dict of the optimizer will be updated.\n",
    "    # It will now contain the moving averages of the gradients and the squared moving averages of the gradients.\n",
    "    print(adam_optimizer.state_dict())\n",
    "    print(\"-\" * 150)\n",
    "    # Update the weights of the neural network using the optimizer.\n",
    "    for name, param in linear_layer.named_parameters():\n",
    "        # Adam optimization.\n",
    "        # m1 and m2 are the first and second moments of the gradients respectively. \n",
    "        # m1 = 0 and m2 = 0 at the start of the optimization.\n",
    "        # m1 = beta_1 * m1 + (1 - beta_1) * param.grad\n",
    "        # m2 = beta_2 * m2 + (1 - beta_2) * (param.grad)^2\n",
    "        # m1_hat = m1 / (1 - (beta_1)^(epoch + 1))\n",
    "        # m2_hat = m2 / (1 - (beta_2)^(epoch + 1))\n",
    "        # param.data = param.data - learning_rate * m1_hat / sqrt(m2_hat)\n",
    "        # param.grad = 0\n",
    "        print(\"shape: \", param.data.shape, \" name: \", name)\n",
    "        print(\"param.data: \", param.data)\n",
    "        print(\"-\" * 150)\n",
    "        print(\"shape: \", param.grad.shape, \" name: \", name)\n",
    "        print(\"param.grad: \", param.grad)\n",
    "        print(\"-\" * 150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".pytorch_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
