{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In this notebook, you learn:\n",
    "# 1) What is learning rate and how is it used in pytorch?\n",
    "#\n",
    "# Resources:\n",
    "# 1) https://www.jeremyjordan.me/nn-learning-rate/\n",
    "#       -- Explains what learning rate is and why is it used.\n",
    "# 2) https://www.youtube.com/watch?v=QzulmoOg2JE&list=PLkDaE6sCZn6Hn0vK8co82zjQtt3T2Nkqc&index=23\n",
    "#       -- Explains what learning rate decay is and why it is used.\n",
    "# 3) https://www.youtube.com/watch?v=81NJgoR5RfY\n",
    "#       -- Gives a walk through of how to use different learning_rate schedulers in pytorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of input features for the linear layer.\n",
    "in_features = 4\n",
    "# Number of output features for the linear layer. \n",
    "out_features = 6\n",
    "# Number of samples in the dataset.\n",
    "num_samples = 4\n",
    "# Hyperparameter to calculate the m1 moment in the optimizer. This roughly corresponds to averaging over the\n",
    "# last 10 (1/(1-beta_1)) sets of gradients. This comes from 'Gradient Descent with Momentum' algorithm.\n",
    "beta_1 = 0.9\n",
    "# Hyperparameter to calculate the m2 moment in the optimizer. This roughly corresponds to averaging over the\n",
    "# last 1000 (1/(1-beta_2)) sets of gradients. This comes from 'RMS prop' algorithm.\n",
    "beta_2 = 0.999\n",
    "# Small value to avoid division by zero in the optimizer.\n",
    "epsilon = 1e-8\n",
    "# Learning rate with which the training is started. This gets updated by the learning rate scheduler periodically.\n",
    "initial_learning_rate = 0.1\n",
    "# Number of iterations for which the training is run.\n",
    "num_iterations = 2\n",
    "# Factor used to decay the learning rate.\n",
    "decay_factor = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape:  torch.Size([4, 4])\n",
      "input_data: \n",
      " tensor([[ 0.3768, -2.0741,  0.9403,  0.1040],\n",
      "        [ 0.3636, -1.2057,  2.1092,  0.8630],\n",
      "        [-1.2130,  1.3250, -0.5797, -0.5042],\n",
      "        [ 0.2132, -0.9422,  0.3078,  0.1567]])\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "shape:  torch.Size([4, 6])\n",
      "output_data: \n",
      " tensor([[ 2.2342,  0.8298, -0.4668,  0.6896, -0.3985,  0.4498],\n",
      "        [-0.2488,  0.1556,  0.6068,  0.7403, -0.0575, -0.0032],\n",
      "        [ 1.2860,  2.0274,  0.7297, -0.0376,  1.1107,  0.6648],\n",
      "        [ 0.8990, -0.4358,  1.3365,  1.8619, -1.1116, -0.9941]])\n"
     ]
    }
   ],
   "source": [
    "# Create random input and output tensors for the neural network.\n",
    "input_data = torch.randn(size=(num_samples, in_features), dtype=torch.float32)\n",
    "print(\"shape: \", input_data.shape)\n",
    "print(\"input_data: \\n\", input_data)\n",
    "print(\"-\" * 150)\n",
    "output_data = torch.randn(size=(num_samples, out_features), dtype=torch.float32)  \n",
    "print(\"shape: \", output_data.shape)\n",
    "print(\"output_data: \\n\", output_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear(in_features=4, out_features=6, bias=True)\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "Adam (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    capturable: False\n",
      "    differentiable: False\n",
      "    eps: 1e-08\n",
      "    foreach: None\n",
      "    fused: None\n",
      "    lr: 0.1\n",
      "    maximize: False\n",
      "    weight_decay: 0\n",
      ")\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "{'state': {}, 'param_groups': [{'lr': 0.1, 'betas': (0.9, 0.999), 'eps': 1e-08, 'weight_decay': 0, 'amsgrad': False, 'maximize': False, 'foreach': None, 'capturable': False, 'differentiable': False, 'fused': None, 'params': [0, 1]}]}\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "MSELoss()\n"
     ]
    }
   ],
   "source": [
    "linear_layer = nn.Linear(4, 6)\n",
    "print(linear_layer)\n",
    "print(\"-\" * 150)\n",
    "# Adam optimizer is a combination of 'Momentum' and 'RMS prop' algorithms. It uses the m1 and m2 moments to update the weights.\n",
    "# Refer to miscellaneous/optimizers.ipynb notebook for more details about adam_optimizer. This will be passed to the \n",
    "# learning rate scheduler to update the learning rate.\n",
    "adam_optimizer = torch.optim.Adam(params=linear_layer.parameters(), lr=initial_learning_rate, betas=(beta_1, beta_2), eps=epsilon)\n",
    "print(adam_optimizer)\n",
    "print(\"-\" * 150)\n",
    "# The learning_rate is '0.1' as it should be since we passed it to the optimizer.\n",
    "print(adam_optimizer.state_dict())\n",
    "print(\"-\" * 150)\n",
    "# Loss function to be used in the neural network (linear layer) back propogation. This is the mean squared error loss function.\n",
    "loss_fn = nn.MSELoss()\n",
    "print(loss_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [torch.optim.lr_scheduler.LambdaLR](https://pytorch.org/docs/stable/generated/torch.optim.lr_scheduler.LambdaLR.html#torch.optim.lr_scheduler.LambdaLR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<function <lambda> at 0x7f9130bc9fc0>\n"
     ]
    }
   ],
   "source": [
    "# learning_rate_updater needs to take an integer as input and returns a float as output for it to be \n",
    "# used with the lr_scheduler. epoch is passed to the learning_rate_updater by the lr_scheduler. We don't\n",
    "# need to pass it explicitly. Also, epoch starts from 0. We will use this function in the below cells.\n",
    "learning_rate_updater = lambda epoch: decay_factor ** epoch\n",
    "print(learning_rate_updater)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'state': {}, 'param_groups': [{'lr': 0.1, 'betas': (0.9, 0.999), 'eps': 1e-08, 'weight_decay': 0, 'amsgrad': False, 'maximize': False, 'foreach': None, 'capturable': False, 'differentiable': False, 'fused': None, 'params': [0, 1]}]}\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "{'state': {}, 'param_groups': [{'lr': 0.2, 'betas': (0.9, 0.999), 'eps': 1e-08, 'weight_decay': 0, 'amsgrad': False, 'maximize': False, 'foreach': None, 'capturable': False, 'differentiable': False, 'fused': None, 'initial_lr': 0.1, 'params': [0, 1]}]}\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "example_learning_rate_updater = lambda epoch: (epoch + 1) + (epoch + 1)\n",
    "example_adam_optimizer = torch.optim.Adam(params=linear_layer.parameters(), lr=initial_learning_rate, betas=(beta_1, beta_2), eps=epsilon)\n",
    "# Note that 'lr' is equal to 0.1 here as expected.\n",
    "print(example_adam_optimizer.state_dict())\n",
    "print(\"-\" * 150)\n",
    "# lr_scheduler takes the optimizer and the learning rate updater function to update the learning rate after every \n",
    "# step (which is controlled by us). Note that the 'lr' in the adam_optimizer is updated in the lr_scheduler \n",
    "# initialization function initially. It is not that the case that 'lr' stays the same as what was passed during \n",
    "# adam_optimizer initialization until the step function of the lr_scheduler is called. However, the next updates \n",
    "# to the 'lr' in the adam_optimizer only happen when you call the step function of the lr_scheduler. Let me \n",
    "# illustrate this with an example below.\n",
    "example_lr_scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer=example_adam_optimizer, lr_lambda=example_learning_rate_updater)\n",
    "# Note that the new 'lr' here is new_lr = lr * ((epoch + 1) + (epoch + 1)) = 0.1 * ((0 + 1) + (0 + 1)) = 0.1 * 2 = 0.2. This\n",
    "# calculation will be explained in the next cell.\n",
    "print(example_adam_optimizer.state_dict())\n",
    "print(\"-\" * 150)\n",
    "# This cell is created just to explain the above behavior. We will not be using the computations in this cell in\n",
    "# the next cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lr_scheduler:  <torch.optim.lr_scheduler.LambdaLR object at 0x7f9133b607f0>\n",
      "last_computed_lr:  [0.1]\n",
      "lr_scheduler_state:  {'base_lrs': [0.1], 'last_epoch': 0, 'verbose': False, '_step_count': 1, '_get_lr_called_within_step': False, '_last_lr': [0.1], 'lr_lambdas': [None]}\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "adam_optimizer_state:  {'state': {}, 'param_groups': [{'lr': 0.1, 'betas': (0.9, 0.999), 'eps': 1e-08, 'weight_decay': 0, 'amsgrad': False, 'maximize': False, 'foreach': None, 'capturable': False, 'differentiable': False, 'fused': None, 'initial_lr': 0.1, 'params': [0, 1]}]}\n"
     ]
    }
   ],
   "source": [
    "# On every epoch, LambdaLR will update the learning rate by calling the 'learning_rate_updater' function and multiplying \n",
    "# the return value of the lambda function with the 'initial_learning_rate' (Note this is 'initial_learning_rate' in every\n",
    "# step and not the learning_rate from the previous step).\n",
    "lr_scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer=adam_optimizer, lr_lambda=learning_rate_updater)\n",
    "print(\"lr_scheduler: \", lr_scheduler)\n",
    "# This gives us the last computed learning rate. This is the learning rate that will be used in the next step.\n",
    "print(\"last_computed_lr: \", lr_scheduler.get_last_lr())\n",
    "print(\"lr_scheduler_state: \", lr_scheduler.state_dict())\n",
    "print(\"-\" * 150)\n",
    "# Here, the 'lr' did not change because decay_factor ** epoch = 0.1 ** 0 = 1.0. So, the 'lr' remains the same as \n",
    "# initial_learning_rate.\n",
    "print(\"adam_optimizer_state: \", adam_optimizer.state_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adam_optimizer_state:  {'state': {0: {'step': tensor(1.), 'exp_avg': tensor([[ 0.0099,  0.0179, -0.0017,  0.0072],\n",
      "        [ 0.0142, -0.0119,  0.0083,  0.0084],\n",
      "        [ 0.0099, -0.0045, -0.0106, -0.0031],\n",
      "        [ 0.0083, -0.0040,  0.0062,  0.0040],\n",
      "        [ 0.0043, -0.0131,  0.0059,  0.0028],\n",
      "        [-0.0202,  0.0641, -0.0579, -0.0205]]), 'exp_avg_sq': tensor([[9.8607e-06, 3.1888e-05, 2.9107e-07, 5.1254e-06],\n",
      "        [2.0112e-05, 1.4069e-05, 6.9440e-06, 7.0562e-06],\n",
      "        [9.7399e-06, 2.0047e-06, 1.1210e-05, 9.5614e-07],\n",
      "        [6.9679e-06, 1.5704e-06, 3.8173e-06, 1.6343e-06],\n",
      "        [1.8718e-06, 1.7196e-05, 3.4965e-06, 7.6975e-07],\n",
      "        [4.0630e-05, 4.1142e-04, 3.3472e-04, 4.1839e-05]])}, 1: {'step': tensor(1.), 'exp_avg': tensor([-0.0326, -0.0092, -0.0254, -0.0169,  0.0087, -0.0278]), 'exp_avg_sq': tensor([1.0655e-04, 8.4636e-06, 6.4480e-05, 2.8613e-05, 7.5459e-06, 7.7210e-05])}}, 'param_groups': [{'lr': 0.1, 'betas': (0.9, 0.999), 'eps': 1e-08, 'weight_decay': 0, 'amsgrad': False, 'maximize': False, 'foreach': None, 'capturable': False, 'differentiable': False, 'fused': None, 'initial_lr': 0.1, 'params': [0, 1]}]}\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "lr_scheduler_state:  {'base_lrs': [0.1], 'last_epoch': 1, 'verbose': False, '_step_count': 2, '_get_lr_called_within_step': False, '_last_lr': [0.010000000000000002], 'lr_lambdas': [None]}\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "adam_optimizer_state:  {'state': {0: {'step': tensor(1.), 'exp_avg': tensor([[ 0.0099,  0.0179, -0.0017,  0.0072],\n",
      "        [ 0.0142, -0.0119,  0.0083,  0.0084],\n",
      "        [ 0.0099, -0.0045, -0.0106, -0.0031],\n",
      "        [ 0.0083, -0.0040,  0.0062,  0.0040],\n",
      "        [ 0.0043, -0.0131,  0.0059,  0.0028],\n",
      "        [-0.0202,  0.0641, -0.0579, -0.0205]]), 'exp_avg_sq': tensor([[9.8607e-06, 3.1888e-05, 2.9107e-07, 5.1254e-06],\n",
      "        [2.0112e-05, 1.4069e-05, 6.9440e-06, 7.0562e-06],\n",
      "        [9.7399e-06, 2.0047e-06, 1.1210e-05, 9.5614e-07],\n",
      "        [6.9679e-06, 1.5704e-06, 3.8173e-06, 1.6343e-06],\n",
      "        [1.8718e-06, 1.7196e-05, 3.4965e-06, 7.6975e-07],\n",
      "        [4.0630e-05, 4.1142e-04, 3.3472e-04, 4.1839e-05]])}, 1: {'step': tensor(1.), 'exp_avg': tensor([-0.0326, -0.0092, -0.0254, -0.0169,  0.0087, -0.0278]), 'exp_avg_sq': tensor([1.0655e-04, 8.4636e-06, 6.4480e-05, 2.8613e-05, 7.5459e-06, 7.7210e-05])}}, 'param_groups': [{'lr': 0.010000000000000002, 'betas': (0.9, 0.999), 'eps': 1e-08, 'weight_decay': 0, 'amsgrad': False, 'maximize': False, 'foreach': None, 'capturable': False, 'differentiable': False, 'fused': None, 'initial_lr': 0.1, 'params': [0, 1]}]}\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "adam_optimizer_state:  {'state': {0: {'step': tensor(2.), 'exp_avg': tensor([[ 0.0204,  0.0241,  0.0050,  0.0160],\n",
      "        [ 0.0203, -0.0091,  0.0053,  0.0112],\n",
      "        [ 0.0169, -0.0078, -0.0173, -0.0052],\n",
      "        [ 0.0092,  0.0059,  0.0012,  0.0029],\n",
      "        [ 0.0020, -0.0066, -0.0040, -0.0005],\n",
      "        [-0.0321,  0.1036, -0.0947, -0.0330]]), 'exp_avg_sq': tensor([[2.3048e-05, 3.8366e-05, 4.6212e-06, 1.4318e-05],\n",
      "        [2.5781e-05, 1.4311e-05, 7.4382e-06, 8.3553e-06],\n",
      "        [1.6200e-05, 3.3880e-06, 1.7266e-05, 1.5265e-06],\n",
      "        [7.2528e-06, 1.0591e-05, 5.7435e-06, 1.6879e-06],\n",
      "        [2.2233e-06, 1.9858e-05, 1.2119e-05, 1.6956e-06],\n",
      "        [6.0051e-05, 6.2125e-04, 5.1632e-04, 6.3219e-05]])}, 1: {'step': tensor(2.), 'exp_avg': tensor([-0.0543, -0.0192, -0.0443, -0.0338,  0.0081, -0.0444]), 'exp_avg_sq': tensor([1.6835e-04, 2.0325e-05, 1.1032e-04, 6.3220e-05, 7.5489e-06, 1.1487e-04])}}, 'param_groups': [{'lr': 0.010000000000000002, 'betas': (0.9, 0.999), 'eps': 1e-08, 'weight_decay': 0, 'amsgrad': False, 'maximize': False, 'foreach': None, 'capturable': False, 'differentiable': False, 'fused': None, 'initial_lr': 0.1, 'params': [0, 1]}]}\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "lr_scheduler_state:  {'base_lrs': [0.1], 'last_epoch': 2, 'verbose': False, '_step_count': 3, '_get_lr_called_within_step': False, '_last_lr': [0.0010000000000000002], 'lr_lambdas': [None]}\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "adam_optimizer_state:  {'state': {0: {'step': tensor(2.), 'exp_avg': tensor([[ 0.0204,  0.0241,  0.0050,  0.0160],\n",
      "        [ 0.0203, -0.0091,  0.0053,  0.0112],\n",
      "        [ 0.0169, -0.0078, -0.0173, -0.0052],\n",
      "        [ 0.0092,  0.0059,  0.0012,  0.0029],\n",
      "        [ 0.0020, -0.0066, -0.0040, -0.0005],\n",
      "        [-0.0321,  0.1036, -0.0947, -0.0330]]), 'exp_avg_sq': tensor([[2.3048e-05, 3.8366e-05, 4.6212e-06, 1.4318e-05],\n",
      "        [2.5781e-05, 1.4311e-05, 7.4382e-06, 8.3553e-06],\n",
      "        [1.6200e-05, 3.3880e-06, 1.7266e-05, 1.5265e-06],\n",
      "        [7.2528e-06, 1.0591e-05, 5.7435e-06, 1.6879e-06],\n",
      "        [2.2233e-06, 1.9858e-05, 1.2119e-05, 1.6956e-06],\n",
      "        [6.0051e-05, 6.2125e-04, 5.1632e-04, 6.3219e-05]])}, 1: {'step': tensor(2.), 'exp_avg': tensor([-0.0543, -0.0192, -0.0443, -0.0338,  0.0081, -0.0444]), 'exp_avg_sq': tensor([1.6835e-04, 2.0325e-05, 1.1032e-04, 6.3220e-05, 7.5489e-06, 1.1487e-04])}}, 'param_groups': [{'lr': 0.0010000000000000002, 'betas': (0.9, 0.999), 'eps': 1e-08, 'weight_decay': 0, 'amsgrad': False, 'maximize': False, 'foreach': None, 'capturable': False, 'differentiable': False, 'fused': None, 'initial_lr': 0.1, 'params': [0, 1]}]}\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# For now lets assume the entire dataset is processed at once and there are no mini-batches.\n",
    "for epoch in range(num_iterations):\n",
    "    # Zero the gradients of the parameters of the neural network.\n",
    "    adam_optimizer.zero_grad()\n",
    "    # Forward pass. Find the output predictions of the neural network.\n",
    "    predictions = linear_layer(input_data)\n",
    "    # Calculate the loss.\n",
    "    loss = loss_fn(predictions, output_data)\n",
    "    # Backward pass. Calculate the gradients of the loss with respect to the parameters of the neural network.\n",
    "    loss.backward()\n",
    "    # Update the weights of the neural network using the optimizer.\n",
    "    adam_optimizer.step()\n",
    "    # The 'lr' should be the same as the value before this 'step' function call. However, the m1 and m2 moments are\n",
    "    # updated in the optimizer. \n",
    "    print(\"adam_optimizer_state: \", adam_optimizer.state_dict())\n",
    "    print(\"-\" * 150)\n",
    "    # The 'lr' is recalculated here by the lr_scheduler and is updated inside the adam_optimizer.\n",
    "    lr_scheduler.step()\n",
    "    print(\"lr_scheduler_state: \", lr_scheduler.state_dict())\n",
    "    print(\"-\" * 150)    \n",
    "    # The 'lr' should be the same as the value you see in the 'lr_scheduler_state' above.\n",
    "    print(\"adam_optimizer_state: \", adam_optimizer.state_dict())\n",
    "    print(\"-\" * 150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".pytorch_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
