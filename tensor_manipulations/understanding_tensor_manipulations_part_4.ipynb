{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In this notebook, you learn:\n",
    "#\n",
    "# 1) How matrix multiplication is carried out with tensors?\n",
    "# 2) How broadcasting works in pytorch / python?\n",
    "# 3) How torch.scatter_ works in pytorch?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [torch.matmul](https://pytorch.org/docs/stable/generated/torch.matmul.html#torch-matmul)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1, 2],\n",
      "        [3, 4]], dtype=torch.int32)\n",
      "torch.Size([2, 2])\n"
     ]
    }
   ],
   "source": [
    "t1 = torch.tensor(data=[[1, 2], [3, 4]], dtype=torch.int)\n",
    "print(t1)\n",
    "print(t1.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[5, 6],\n",
      "        [7, 8]], dtype=torch.int32)\n",
      "torch.Size([2, 2])\n"
     ]
    }
   ],
   "source": [
    "t2 = torch.tensor(data=[[5, 6], [7, 8]], dtype=torch.int)\n",
    "print(t2)\n",
    "print(t2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[19, 22],\n",
      "        [43, 50]], dtype=torch.int32) torch.Size([2, 2]) \n",
      "\n",
      "tensor([[19, 22],\n",
      "        [43, 50]], dtype=torch.int32) torch.Size([2, 2]) \n",
      "\n",
      "Both t3 and t4 tensors contain the same values i.e., t3 = t4\n"
     ]
    }
   ],
   "source": [
    "# '@' is a short hand notation for matrix muplitlication. It performs the same operations as matmul.\n",
    "# \n",
    "# This peforms the following matrix multiplication:\n",
    "# |1, 2| x |5, 6| = |19 22|\n",
    "# |3, 4|   |7, 8|   |43 50|\n",
    "t3 = t1 @ t2\n",
    "t4 = torch.matmul(t1, t2)\n",
    "print(t3, t3.shape, \"\\n\")\n",
    "print(t4, t4.shape, \"\\n\")\n",
    "\n",
    "if torch.equal(t3, t4):\n",
    "    print(\"Both t3 and t4 tensors contain the same values i.e., t3 = t4\")\n",
    "else:\n",
    "    print(\"Elements in t3 and different from elements in t4 i.e., t3 != t4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 9, 10], dtype=torch.int32)\n",
      "torch.Size([2])\n"
     ]
    }
   ],
   "source": [
    "# 1D tensors are basically vectors.\n",
    "t5 = torch.tensor(data=[9, 10], dtype=torch.int)\n",
    "print(t5)\n",
    "print(t5.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([29, 67], dtype=torch.int32)\n",
      "torch.Size([2])\n"
     ]
    }
   ],
   "source": [
    "# '@' operator performs matrix-vector multiplication when one of the operators is a 1D tensor(vector).\n",
    "#\n",
    "# This performs the following matrix-vector multiplication:\n",
    "# |1, 2| x |9 | = |29|\n",
    "# |3, 4|   |10|   |67|\n",
    "t6 = t1 @ t5\n",
    "print(t6)\n",
    "print(t6.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([11, 12], dtype=torch.int32)\n",
      "torch.Size([2])\n"
     ]
    }
   ],
   "source": [
    "t7 = torch.tensor(data=[11, 12], dtype=torch.int)\n",
    "print(t7)\n",
    "print(t7.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(219, dtype=torch.int32)\n",
      "torch.Size([])\n"
     ]
    }
   ],
   "source": [
    "# '@' performs dot-product when both the operators are 1D tensors (vector).\n",
    "#\n",
    "# This performs the following dot-product:\n",
    "# |9, 10| . |11, 12| = 219\n",
    "t8 = t5 @ t7\n",
    "print(t8)\n",
    "print(t8.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Broadcasting in pytorch](https://pytorch.org/docs/stable/notes/broadcasting.html#broadcasting-semantics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resources to go through to understanding broadcasting before continuing further in this notebook:\n",
    "# https://www.youtube.com/watch?v=tKcLaGdvabM \n",
    "#       -- Explains what broadcasting is and how it works on matrices with examples.\n",
    "# https://pytorch.org/docs/stable/notes/broadcasting.html#broadcasting-semantics\n",
    "#       -- Explains how shape of the input tensors and resultant tensors get manipulated during broadcasting.\n",
    "#\n",
    "# Copied the below conditions from pytorch official documentation:\n",
    "#\n",
    "# Two tensors are broadcastable if the following conditions hold:\n",
    "# 1) Each tensor has at least one dimension.\n",
    "# 2) When iterating over the dimension sizes, starting at the trailing dimension (last dimension), \n",
    "#    the dimension sizes must either be equal, one of them is 1, or one of them does not exist.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 1.,  2.,  3.,  4.],\n",
      "         [ 5.,  6.,  7.,  8.]],\n",
      "\n",
      "        [[ 9., 10., 11., 12.],\n",
      "         [13., 14., 15., 16.]]])\n",
      "torch.Size([2, 2, 4])\n"
     ]
    }
   ],
   "source": [
    "t9 = torch.tensor(data=[[[1, 2, 3, 4], [5, 6, 7, 8]], [[9, 10, 11, 12], [13, 14, 15, 16]]], dtype=torch.float)\n",
    "print(t9)\n",
    "print(t9.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([2., 3., 4., 5.])\n",
      "torch.Size([4])\n"
     ]
    }
   ],
   "source": [
    "# Recall from the 'understanding_tensors.ipynb' notebook that '1D' tensors are neither column\n",
    "# vector nor row vectors. They behave according to the context they are used in.\n",
    "t10 = torch.tensor(data=[2, 3, 4, 5], dtype=torch.float)\n",
    "print(t10)\n",
    "print(t10.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 2.,  6., 12., 20.],\n",
      "         [10., 18., 28., 40.]],\n",
      "\n",
      "        [[18., 30., 44., 60.],\n",
      "         [26., 42., 60., 80.]]])\n",
      "torch.Size([2, 2, 4])\n"
     ]
    }
   ],
   "source": [
    "# Here '*' is a simple element by element multiplication operation. However, the shapes of t9 (2, 2, 4)\n",
    "# and t10 (4) are not the same. So, python broadcasting does it magic here to bring both the tensors\n",
    "# to the same shape before applying the multiplication operation.\n",
    "# \n",
    "# In this context t10 ([2, 3, 4, 5]) behaves as a row vector. Lets find out how the shape of the result (t11)\n",
    "# is obtained from the shapes of t9, t10 as explained in pytorch official documentation (link above).\n",
    "# t9  --> (2, 2, 4)\n",
    "# t10 --> (_, _, 4)\n",
    "# t11 --> (2, 2, 4)\n",
    "#\n",
    "# Steps involved in broadcasting:\n",
    "# 1) Size (4) of Dimension 2 is same for both tensors t9, t10. So, not changes here. \n",
    "# 2) Dimension 1 of t10 needs to be made of size 2. So, the 1D tensor is copied to give the 2D tensor.\n",
    "#           -- [2, 3, 4, 5] of t10 gets broadcasted to the 2D tensor [[2, 3, 4, 5], [2, 3, 4, 5]]   \n",
    "# 3) Dimension 0 of t10 needs to made of size 2. So, the 2D tensor is copied to give the 3D tensor.\n",
    "#           -- [[2, 3, 4, 5], [2, 3, 4, 5]] gets broadcasted to the 3D tensor [[[2, 3, 4, 5], [2, 3, 4, 5]], [[2, 3, 4, 5], [2, 3, 4, 5]]]\n",
    "# \n",
    "# Now element wise multiplication happens between the two tensors to give t11.\n",
    "# [[[1 * 2, 2 * 3, 3 * 4, 4 * 5], [5 * 2, 6 * 3, 7 * 4, 8 * 5]], [[9 * 2, 10 * 3, 11 * 4, 12 * 5], [13 * 2, 14 * 3, 15 * 4, 16 * 11]]]\n",
    "# The left elements belong to t9 (after broadcasting) and right elements belong to t10 (after broadcasting) in each\n",
    "# multiplication.\n",
    "t11 = t9 * t10\n",
    "print(t11)\n",
    "print(t11.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[10.],\n",
      "         [20.]]])\n",
      "torch.Size([1, 2, 1])\n"
     ]
    }
   ],
   "source": [
    "t12 = torch.tensor(data=[[[10], [20]]], dtype=torch.float)\n",
    "print(t12)\n",
    "print(t12.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[11., 12., 13., 14.],\n",
      "         [25., 26., 27., 28.]],\n",
      "\n",
      "        [[19., 20., 21., 22.],\n",
      "         [33., 34., 35., 36.]]])\n",
      "torch.Size([2, 2, 4])\n"
     ]
    }
   ],
   "source": [
    "# Here '+' is a simple element by element addition operation. However, the shapes of t9 (2, 2, 4)\n",
    "# and t10 (1, 2, 1) are not the same. So, python broadcasting does it magic here to bring both the tensors\n",
    "# to the same shape before applying the addition operation. \n",
    "#\n",
    "# Lets find out how the shape of the result (t13) is obtained from the the shapes of t9, t12 according\n",
    "# to the rules in the pytorch official documentation (link above).\n",
    "# t9  --> (2, 2, 4)\n",
    "# t12 --> (1, 2, 1)\n",
    "# t13 --> (2, 2, 4)\n",
    "#\n",
    "# Steps involved in broadcasting:\n",
    "# 1) Dimension 2 of t12 needs to be made 4. So, the element in each 1D tensor in the last dimension is broadcasted (copied).\n",
    "#           -- [10] --> [10, 10, 10, 10]\n",
    "#           -- [20] --> [20, 20, 20, 20]\n",
    "#           -- So, [[[10], [20]]] gets broadcasted to [[[10, 10, 10, 10], [20, 20, 20, 20]]]]\n",
    "# 2) Dimension 1 is already 2 in both the tensors (t9, t10). So, no changes here.\n",
    "# 3) Dimension 0 of t12 needs to be made of size 2. So, the 2D tensor gets copied to give the 3D tensor.\n",
    "#           -- [[[10, 10, 10, 10], [20, 20, 20, 20]]] gets broadcasted to [[[10, 10, 10, 10], [20, 20, 20, 20]], [[10, 10, 10, 10], [20, 20, 20, 20]]]\n",
    "#\n",
    "# Now, element wise addition happens between the two tensors to give t13.\n",
    "# The left elements belong to t9 (after broadcasting) and right elements belong to t12 (after broadcasting) in each\n",
    "# addition.\n",
    "t13 = t9 + t12\n",
    "print(t13)\n",
    "print(t13.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 0,  1,  2,  3],\n",
      "         [ 4,  5,  6,  7],\n",
      "         [ 8,  9, 10, 11]]])\n",
      "torch.Size([1, 3, 4])\n"
     ]
    }
   ],
   "source": [
    "# Now lets take an example when both the tensors need to updated because of the broadcast. In the \n",
    "# above examples, only the right tensor always got broadcasted.\n",
    "t14 = torch.arange(end=12).reshape(1, 3, 4)\n",
    "print(t14)\n",
    "print(t14.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[ 0,  1,  2,  3]],\n",
      "\n",
      "         [[ 4,  5,  6,  7]]],\n",
      "\n",
      "\n",
      "        [[[ 8,  9, 10, 11]],\n",
      "\n",
      "         [[12, 13, 14, 15]]]])\n",
      "torch.Size([2, 2, 1, 4])\n"
     ]
    }
   ],
   "source": [
    "t15 = torch.arange(16).reshape(2, 2, 1, 4)\n",
    "print(t15)\n",
    "print(t15.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[ 0,  2,  4,  6],\n",
      "          [ 4,  6,  8, 10],\n",
      "          [ 8, 10, 12, 14]],\n",
      "\n",
      "         [[ 4,  6,  8, 10],\n",
      "          [ 8, 10, 12, 14],\n",
      "          [12, 14, 16, 18]]],\n",
      "\n",
      "\n",
      "        [[[ 8, 10, 12, 14],\n",
      "          [12, 14, 16, 18],\n",
      "          [16, 18, 20, 22]],\n",
      "\n",
      "         [[12, 14, 16, 18],\n",
      "          [16, 18, 20, 22],\n",
      "          [20, 22, 24, 26]]]])\n",
      "torch.Size([2, 2, 3, 4])\n"
     ]
    }
   ],
   "source": [
    "# Here '+' is a simple element by element addition operation. However, the shapes of t14 (1, 3, 4)\n",
    "# and t15 (2, 2, 1, 4) are not the same. So, python broadcasting does it magic here to bring both the tensors\n",
    "# to the same shape before applying the addition operation. \n",
    "#\n",
    "# Lets find out how the shape of the result (t16) is obtained from the the shapes of t14, t15 according to\n",
    "# the rules in the pytorch official documentation (link above).\n",
    "# t14 --> (_, 1, 3, 4)\n",
    "# t15 --> (2, 2, 1, 4)\n",
    "# t16 --> (2, 2, 3, 4)\n",
    "#\n",
    "# Steps involved in broadcasting:\n",
    "# 1) No changes in dimension 3 since both the tensors have the same size in dimension 4.\n",
    "# 2) Dimension 2 of t15 needs to be made of size 3. So, the 1D tensor along dimension 4 gets copied 3 times to give 2D tensor.\n",
    "#           -- [0, 1, 2, 3]     --> [[0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3]]\n",
    "#           -- [4, 5, 6, 7]     --> [[4, 5, 6, 7], [4, 5, 6, 7], [4, 5, 6, 7]]\n",
    "#           -- [8, 9, 10, 11]   --> [[8, 9, 10, 11], [8, 9, 10, 11], [8, 9, 10, 11]]\n",
    "#           -- [12, 13, 14, 15] --> [[12, 13, 14, 15], [12, 13, 14, 15], [12, 13, 14, 15]]\n",
    "# 3) Dimension 1 of t14 needs to be made 2. So, the 2D tensor gets copied to create a 3D tensor.\n",
    "#           -- [[0, 1, 2, 3], [4, 5, 6, 7], [8, 9, 10, 11]] --> [[[0, 1, 2, 3], [4, 5, 6, 7], [8, 9, 10, 11]], [[0, 1, 2, 3], [4, 5, 6, 7], [8, 9, 10, 11]]]\n",
    "# 4) Dimension 0 of t14 needs to be made 2. So, the 3D tensor gets copied to created a 4D tensor.\n",
    "#           -- [[[[0, 1, 2, 3], [4, 5, 6, 7], [8, 9, 10, 11]], [[0, 1, 2, 3], [4, 5, 6, 7], [8, 9, 10, 11]]], [[[0, 1, 2, 3], [4, 5, 6, 7], [8, 9, 10, 11]], [[0, 1, 2, 3], [4, 5, 6, 7], [8, 9, 10, 11]]]]\n",
    "# \n",
    "# Now, element wise addition happens between the two tensors to give t16.\n",
    "# The left elements belong to t14 (after broadcasting) and right elements belong to t15 (after broadcasting) in each\n",
    "# addition.\n",
    "t16 = t14 + t15\n",
    "print(t16)\n",
    "print(t16.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [torch.scatter_](https://pytorch.org/docs/stable/generated/torch.Tensor.scatter_.html#torch-tensor-scatter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Excellent resource to understand scatter:\n",
    "# 1) https://yuyangyy.medium.com/understand-torch-scatter-b0fd6275331c\n",
    "#       -- Please go through this link to understand scatter in detail with examples before continuing further.\n",
    "# \n",
    "# _ at the end of 'scatter_' indicates that this is an in-place operation. \n",
    "# scatter basically scatters the inputs from the src tensor to the destination tensor according to the index tensor.\n",
    "# The index tensor is used to determine the position in the destination tensor where the elements from the src tensor\n",
    "# need to be placed. The src, index and destionation tensors need to have the same number of dimensions. Note that\n",
    "# it is the same number of dimensions and not the same shape."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape:  torch.Size([2, 5])\n",
      "src: \n",
      " tensor([[ 1,  2,  3,  4,  5],\n",
      "        [ 6,  7,  8,  9, 10]])\n"
     ]
    }
   ],
   "source": [
    "# Lets first consider the case when the src is a tensor.\n",
    "src = torch.arange(start=1, end=11).reshape(2, 5)\n",
    "print(\"shape: \", src.shape)\n",
    "print(\"src: \\n\", src)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape:  torch.Size([1, 4])\n",
      "index: \n",
      " tensor([[0, 1, 2, 0]])\n"
     ]
    }
   ],
   "source": [
    "# Note that index is a 2D tensor like src tensor. However, the shape of the index tensor is different from the src tensor.\n",
    "index = torch.tensor(data=[[0, 1, 2, 0]], dtype=torch.int64)\n",
    "print(\"shape: \", index.shape)\n",
    "print(\"index: \\n\", index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape:  torch.Size([3, 5])\n",
      "destination: \n",
      " tensor([[0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0]])\n"
     ]
    }
   ],
   "source": [
    "destination = torch.zeros(size=(3, 5), dtype=torch.int64)\n",
    "print(\"shape: \", destination.shape)\n",
    "print(\"destination: \\n\", destination)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape:  torch.Size([3, 5])\n",
      "destination: \n",
      " tensor([[1, 0, 0, 4, 0],\n",
      "        [0, 2, 0, 0, 0],\n",
      "        [0, 0, 3, 0, 0]])\n"
     ]
    }
   ],
   "source": [
    "# src tensor is [[1, 2, 3, 4, 5], [6, 7, 8, 9, 10]].\n",
    "# index tensor is [[0, 1, 2, 0]].\n",
    "#\n",
    "# Now lets go to each element in the src tensor and place it in the destination tensor according to the index tensor.\n",
    "# src[0][0] = 1 is placed in destination[index[0][0]][0] = destiantion[0][0] => destination[0][0] = 1\n",
    "# src[0][1] = 2 is placed in destination[index[0][1]][1] = destiantion[1][1] => destination[1][1] = 2\n",
    "# src[0][2] = 3 is placed in destination[index[0][2]][2] = destiantion[2][2] => destination[2][2] = 3\n",
    "# src[0][3] = 4 is placed in destination[index[0][3]][3] = destiantion[0][3] => destination[0][3] = 4\n",
    "#\n",
    "# src[0][4] = 5. But notice that index doesn't have any value at position [0][4]. So, src[0][4] = 5 is not placed\n",
    "# in the destination tensor at all. So, destination[0][4] = 0.\n",
    "# In general, if the index tensor doesn't have a value at a particular position, the corresponding element in the \n",
    "# src tensor is not placed in the destination tensor.\n",
    "#\n",
    "# Following the above argument, no other elements (except the ones shown above) from the src tensor are placed in the \n",
    "# destination tensor.\n",
    "destination.scatter_(dim=0, index=index, src=src)\n",
    "print(\"shape: \", destination.shape)\n",
    "print(\"destination: \\n\", destination)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To have an intuitive understanding, scatter_ can be thought of as a way to scatter the elements from the src tensor\n",
    "# to the destination tensor in a specific dimension. Lets say the destination tensor is a 2D tensor. If scatter_\n",
    "# is used to scatter the elements along dimension 0, it means that the elements in a specific column in the src tensor\n",
    "# are only scattered to other positions in the same column in the destination tensor. \n",
    "# \n",
    "# Consider the scatter_ formula --> destination[index[i][j]][j] = src[i][j] for all i, j.\n",
    "# The elements in column j of the src tensor always stay in column j of the destination tensor. The only thing that\n",
    "# changes is the row position of the elements in column j of the destination tensor. So, scatter_ has a specific \n",
    "# pattern intuitively on how it scatters the elements from the src tensor to the destination tensor. You can \n",
    "# extrapolate the same intuition to a different dimension (that is not zero) or higher dimensions as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now, lets consider the case when the src is a scalar.\n",
    "scalar_src = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape:  torch.Size([1, 4])\n",
      "index: \n",
      " tensor([[0, 1, 2, 0]])\n"
     ]
    }
   ],
   "source": [
    "index = torch.tensor(data=[[0, 1, 2, 0]], dtype=torch.int64)\n",
    "print(\"shape: \", index.shape)\n",
    "print(\"index: \\n\", index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape:  torch.Size([3, 5])\n",
      "destination: \n",
      " tensor([[0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0]])\n"
     ]
    }
   ],
   "source": [
    "destination = torch.zeros(size=(3, 5), dtype=torch.int64)\n",
    "print(\"shape: \", destination.shape)\n",
    "print(\"destination: \\n\", destination)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape:  torch.Size([3, 5])\n",
      "destination: \n",
      " tensor([[100,   0,   0, 100,   0],\n",
      "        [  0, 100,   0,   0,   0],\n",
      "        [  0,   0, 100,   0,   0]])\n"
     ]
    }
   ],
   "source": [
    "# src is a scalar value 100. The index tensor is the same as before.\n",
    "# index = [[0, 1, 2, 0]]\n",
    "# \n",
    "# Now lets go to each element in the index tensor and place the scalar value in the destination tensor according \n",
    "# to the index tensor. When the 'src' is a scalar, the scalar value is placed in the destination tensor at the\n",
    "# positions specified by the index tensor. We don't look at the src tensor at all to find the appropriate indices\n",
    "# in the destination tensor. \n",
    "# \n",
    "# The general formula for scatter_ when src is a scalar is --> destination[index[i][j]][j] = scalar_src for all i, j.\n",
    "# index[0][0] = 0 is used to place the src at destination[index[0][0]][0] = destiantion[0][0] => destination[0][0] = 100\n",
    "# index[0][1] = 1 is used to place the src at destination[index[0][1]][1] = destiantion[1][1] => destination[1][1] = 100    \n",
    "# index[0][2] = 2 is used to place the src at destination[index[0][2]][2] = destiantion[2][2] => destination[2][2] = 100\n",
    "# index[0][3] = 0 is used to place the src at destination[index[0][3]][3] = destiantion[0][3] => destination[0][3] = 100\n",
    "# \n",
    "# These are the only indices at which index is defined. So, the scalar value is placed at only these positions.\n",
    "destination.scatter_(dim=0, index=index, value=scalar_src)\n",
    "print(\"shape: \", destination.shape)\n",
    "print(\"destination: \\n\", destination)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".pytorch_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
