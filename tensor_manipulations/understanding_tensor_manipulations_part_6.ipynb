{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In this notebook, you learn:\n",
    "#\n",
    "# 1) How torch.reshape works?\n",
    "# 2) How torch.view works?\n",
    "# 3) How torch.transpose works?\n",
    "# 4) How torch.repeat works?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resources to go through before continuining further in this notebook:\n",
    "#\n",
    "# 1) tensor_manipulations/understanding_tensors_part_2.ipynb\n",
    "#       -- Explains how stride, contiguity and underlying storage works in a tensor.\n",
    "# 2) https://dzone.com/articles/reshaping-pytorch-tensors\n",
    "#       -- Explains how reshape and view work using stride and contiguous properties."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [torch.reshape](https://pytorch.org/docs/stable/generated/torch.reshape.html#torch-reshape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One of the most confusing things when I tried to understand how reshape works is figuring out\n",
    "# how the elements from the original tensor are rearranged in the reshaped tensor. To understand \n",
    "# this, you need to know how the tensors are stored internally in the memory. Please refer to\n",
    "# 'tensor_manipulations/understanding_tensors_part_2.ipynb' notebook to understand that before \n",
    "# continuining further."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t1:  tensor([[ 1,  2,  3,  4,  5],\n",
      "        [ 6,  7,  8,  9, 10],\n",
      "        [11, 12, 13, 14, 15],\n",
      "        [16, 17, 18, 19, 20]])\n",
      "shape:  torch.Size([4, 5])\n"
     ]
    }
   ],
   "source": [
    "t1 = torch.tensor(data=[[1, 2, 3, 4, 5], [6, 7, 8, 9, 10], [11, 12, 13, 14, 15], [16, 17, 18, 19, 20]], dtype=torch.int64)\n",
    "print(\"t1: \", t1)\n",
    "print(\"shape: \", t1.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "is_contiguous:  True\n",
      "storage:   1\n",
      " 2\n",
      " 3\n",
      " 4\n",
      " 5\n",
      " 6\n",
      " 7\n",
      " 8\n",
      " 9\n",
      " 10\n",
      " 11\n",
      " 12\n",
      " 13\n",
      " 14\n",
      " 15\n",
      " 16\n",
      " 17\n",
      " 18\n",
      " 19\n",
      " 20\n",
      "[torch.storage.TypedStorage(dtype=torch.int64, device=cpu) of size 20]\n"
     ]
    }
   ],
   "source": [
    "print(\"is_contiguous: \", t1.is_contiguous())\n",
    "t1_storage = t1.storage()\n",
    "# This shows the elements in the underlying storage.\n",
    "print(\"storage: \", t1_storage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t2:  tensor([[ 1,  2],\n",
      "        [ 3,  4],\n",
      "        [ 5,  6],\n",
      "        [ 7,  8],\n",
      "        [ 9, 10],\n",
      "        [11, 12],\n",
      "        [13, 14],\n",
      "        [15, 16],\n",
      "        [17, 18],\n",
      "        [19, 20]])\n",
      "shape:  torch.Size([10, 2])\n",
      "stride:  (2, 1)\n"
     ]
    }
   ],
   "source": [
    "# Notice the placing of the elements in t2. The elements from the tensor (t1) are taken sequentially \n",
    "# and put into the new tensor (t2). This is equivalent to iterating on the storage of t1, taking each \n",
    "# element and the filling the positions in t2 sequentially i.e., filling the indices in the order\n",
    "# (0, 0); (0, 1); (1, 0); (1, 1); (2, 0): (2, 1); ... (9, 0); (9, 1)\n",
    "# \n",
    "# When the underlying storage holds the elements contiguously, the sequence order in tensor t2 is \n",
    "# same as the order of the elements in the storage. If the elements are not hold contiguously, then \n",
    "# the sequence order used for reshape is determined by the order of the elements in t2 and not the \n",
    "# storage (examples for this below). In this example, the storage order is good to compute reshape.\n",
    "t2 = t1.reshape(10, 2)\n",
    "print(\"t2: \", t2)\n",
    "print(\"shape: \", t2.shape)\n",
    "print(\"stride: \", t2.stride())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Both the tensors t1 and t2 have same underlying storage\n"
     ]
    }
   ],
   "source": [
    "# reshape did not copy the underlying storage for the new tensor (t2) since the new shape\n",
    "# is compatible with the existing storage. The new tensor (view) can be obtained by just \n",
    "# adjusting the stride within the new tensor (t2) using the same storage.\n",
    "if t1.storage().data_ptr() == t2.storage().data_ptr():\n",
    "    print(\"Both the tensors t1 and t2 have same underlying storage\")\n",
    "else:\n",
    "    print(\"The tensors t1 and t2 do not share the same storage\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t3:  tensor([[ 1,  6, 11, 16],\n",
      "        [ 2,  7, 12, 17],\n",
      "        [ 3,  8, 13, 18],\n",
      "        [ 4,  9, 14, 19],\n",
      "        [ 5, 10, 15, 20]])\n",
      "shape:  torch.Size([5, 4])\n",
      "stride:  (1, 5)\n",
      "storage:   1\n",
      " 2\n",
      " 3\n",
      " 4\n",
      " 5\n",
      " 6\n",
      " 7\n",
      " 8\n",
      " 9\n",
      " 10\n",
      " 11\n",
      " 12\n",
      " 13\n",
      " 14\n",
      " 15\n",
      " 16\n",
      " 17\n",
      " 18\n",
      " 19\n",
      " 20\n",
      "[torch.storage.TypedStorage(dtype=torch.int64, device=cpu) of size 20]\n",
      "is_contiguous:  False\n",
      "Both the tensors t1 and t3 have same underlying storage\n"
     ]
    }
   ],
   "source": [
    "t3 = t1.transpose(0, 1)\n",
    "print(\"t3: \", t3)\n",
    "print(\"shape: \", t3.shape)\n",
    "print(\"stride: \", t3.stride())\n",
    "print(\"storage: \", t3.storage())\n",
    "# is_contiguous is False because the elements in the tensor t3 are not stored sequentially \n",
    "# (contiguously) in the underlying storage.\n",
    "#\n",
    "# The order of the elements in the tensor t3 is:\n",
    "# [1, 6, 11, 16, 2, 7, 12, 17, 3, 8, 13, 18, 4, 9, 14, 19, 5, 10, 15, 20].\n",
    "#\n",
    "# The sequence order of elements for any tensor can be determined by:\n",
    "# 1) Get the tensor at index 0 in dimension 0. \n",
    "# 2) Keep traversing it recursively to get first 1D tensor.\n",
    "# 3) List all the elements in this 1D tensor.\n",
    "# 4) Go to the next 1D tensor and list all its elements.\n",
    "# 5) Repeat this process until all the elements in the tensor at index 0 in dimension 0 are exhausted.\n",
    "# 6) Repeat this process for all tensors in dimension 0. \n",
    "#\n",
    "# At the end of this process, we have a single 1D array of elements which is the required order of elements.\n",
    "#\n",
    "# The order of the elements in the storage for t3 is:\n",
    "# [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20]\n",
    "print(\"is_contiguous: \", t3.is_contiguous())\n",
    "if t1.storage().data_ptr() == t3.storage().data_ptr():\n",
    "    print(\"Both the tensors t1 and t3 have same underlying storage\")\n",
    "else:\n",
    "    print(\"The tensors t1 and t3 do not share the same storage\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t4:  tensor([[ 1,  6],\n",
      "        [11, 16],\n",
      "        [ 2,  7],\n",
      "        [12, 17],\n",
      "        [ 3,  8],\n",
      "        [13, 18],\n",
      "        [ 4,  9],\n",
      "        [14, 19],\n",
      "        [ 5, 10],\n",
      "        [15, 20]])\n",
      "shape:  torch.Size([10, 2])\n",
      "stride:  (2, 1)\n",
      "storage:   1\n",
      " 6\n",
      " 11\n",
      " 16\n",
      " 2\n",
      " 7\n",
      " 12\n",
      " 17\n",
      " 3\n",
      " 8\n",
      " 13\n",
      " 18\n",
      " 4\n",
      " 9\n",
      " 14\n",
      " 19\n",
      " 5\n",
      " 10\n",
      " 15\n",
      " 20\n",
      "[torch.storage.TypedStorage(dtype=torch.int64, device=cpu) of size 20]\n",
      "is_contiguous:  True\n",
      "The tensors t3 and t4 do not share the same storage\n"
     ]
    }
   ],
   "source": [
    "# Notice the order of the elements in the reshaped tensor (t4). The order is based on the order\n",
    "# of the elements in the tensor (t3) and not its underlying storage. \n",
    "#\n",
    "# The order of the elements in the tensor t3 is defined by its stride. As explained above, you start \n",
    "# iterating from the outer-most dimension and go inside until the last dimension and list all the \n",
    "# elements along this last dimension. You then go to the next tensor and repeat this process until \n",
    "# all the elements in the tensor are accounted. The resultant flattened 1D array is then reshaped \n",
    "# into the new tensor.\n",
    "t4 = t3.reshape(10, 2)\n",
    "print(\"t4: \", t4)\n",
    "print(\"shape: \", t4.shape)\n",
    "print(\"stride: \", t4.stride())\n",
    "print(\"storage: \", t4.storage())\n",
    "print(\"is_contiguous: \", t4.is_contiguous())\n",
    "# A new storage is created for t4 since t3 is not-contiguous and the new shape doesn't align with\n",
    "# the original storage. In general, you cannot depend on reshape to create / not create a new storage.\n",
    "# reshape first tries to do a view operation if possible which does not create a copy of the storage.\n",
    "# However, if the view operation is not possible, it creates a new storage and does the reshaping \n",
    "# appropriately.\n",
    "if t3.storage().data_ptr() == t4.storage().data_ptr():\n",
    "    print(\"Both the tensors t3 and t4 have same underlying storage\")\n",
    "else:\n",
    "    print(\"The tensors t3 and t4 do not share the same storage\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t5:  tensor([[ 1,  6, 11, 16],\n",
      "        [ 2,  7, 12, 17],\n",
      "        [ 3,  8, 13, 18],\n",
      "        [ 4,  9, 14, 19],\n",
      "        [ 5, 10, 15, 20]])\n",
      "shape:  torch.Size([5, 4])\n",
      "stride:  (4, 1)\n",
      "storage:   1\n",
      " 6\n",
      " 11\n",
      " 16\n",
      " 2\n",
      " 7\n",
      " 12\n",
      " 17\n",
      " 3\n",
      " 8\n",
      " 13\n",
      " 18\n",
      " 4\n",
      " 9\n",
      " 14\n",
      " 19\n",
      " 5\n",
      " 10\n",
      " 15\n",
      " 20\n",
      "[torch.storage.TypedStorage(dtype=torch.int64, device=cpu) of size 20]\n"
     ]
    }
   ],
   "source": [
    "# This returns a new tensor (t5) same as t3 but with the underlying storage contiguous in t5 unlike t3.\n",
    "# Notice that the order of the elements in the underlying storage for t5 is same as t4. \n",
    "# In general, if it gets harder to determine the order of the elements used by reshape operation, simply\n",
    "# make the original tensor contiguous and figure out the order by printing its storage.\n",
    "t5 = t3.contiguous()\n",
    "print(\"t5: \", t5)\n",
    "print(\"shape: \", t5.shape)\n",
    "print(\"stride: \", t5.stride())\n",
    "print(\"storage: \", t5.storage())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [tensor.view](https://pytorch.org/docs/stable/generated/torch.Tensor.view.html#torch-tensor-view)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Please go through the reshape operation first before understanding the view operation.\n",
    "# The order of the elements in the new tensor after view opeartion is determined in the same way \n",
    "# as we did for reshape operation. The only difference is that view does not create a new storage \n",
    "# for the new tensor. If it cannot create a view without changing the storage, it throws an error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t6:  tensor([[ 1,  2,  3,  4],\n",
      "        [ 5,  6,  7,  8],\n",
      "        [ 9, 10, 11, 12]])\n",
      "shape:  torch.Size([3, 4])\n",
      "stride:  (4, 1)\n",
      "is_contiguous:  True\n",
      "storage:   1\n",
      " 2\n",
      " 3\n",
      " 4\n",
      " 5\n",
      " 6\n",
      " 7\n",
      " 8\n",
      " 9\n",
      " 10\n",
      " 11\n",
      " 12\n",
      "[torch.storage.TypedStorage(dtype=torch.int64, device=cpu) of size 12]\n"
     ]
    }
   ],
   "source": [
    "t6 = torch.tensor(data=[[1, 2, 3, 4], [5, 6, 7, 8], [9, 10, 11, 12]], dtype=torch.int64)\n",
    "print(\"t6: \", t6)\n",
    "print(\"shape: \", t6.shape)\n",
    "print(\"stride: \", t6.stride())\n",
    "print(\"is_contiguous: \", t6.is_contiguous())\n",
    "print(\"storage: \", t6.storage())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I don't actually understand the condition mentioned in the official documentation for the\n",
    "# view operation to be valid. From, what I understand, all we need is that the original \n",
    "# tensor need to contiguous for view operation to be valid. Please let me know if this is\n",
    "# not the case.\n",
    "#\n",
    "# Apparently, a view operation can be applied even if the tensor is not contiguous. It only\n",
    "# needs to satisfy a loose contiguity-like condition. This is probably what the official\n",
    "# pytorch documentation is explaining. (Example for this below)\n",
    "#\n",
    "# 1) https://kamilelukosiute.com/pytorch/When+can+a+tensor+be+view()ed%3F\n",
    "#       -- Explains the necessary conditions for a view operation to be valid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t7:  tensor([[ 1,  2,  3],\n",
      "        [ 4,  5,  6],\n",
      "        [ 7,  8,  9],\n",
      "        [10, 11, 12]])\n",
      "shape:  torch.Size([4, 3])\n",
      "stride:  (3, 1)\n",
      "is_contiguous:  True\n"
     ]
    }
   ],
   "source": [
    "t7 = t6.view(4, 3)\n",
    "print(\"t7: \", t7)\n",
    "print(\"shape: \", t7.shape)\n",
    "print(\"stride: \", t7.stride())\n",
    "print(\"is_contiguous: \", t7.is_contiguous())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t8:  tensor([[ 1,  2,  3,  4,  5,  6],\n",
      "        [ 7,  8,  9, 10, 11, 12]])\n",
      "shape:  torch.Size([2, 6])\n",
      "stride:  (6, 1)\n",
      "is_contiguous:  True\n"
     ]
    }
   ],
   "source": [
    "t8 = t6.view(2, 6)\n",
    "print(\"t8: \", t8)\n",
    "print(\"shape: \", t8.shape)\n",
    "print(\"stride: \", t8.stride())\n",
    "print(\"is_contiguous: \", t8.is_contiguous())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t10:  tensor([[[ 1,  2],\n",
      "         [ 3,  4],\n",
      "         [ 5,  6]],\n",
      "\n",
      "        [[ 7,  8],\n",
      "         [ 9, 10],\n",
      "         [11, 12]]])\n",
      "shape:  torch.Size([2, 3, 2])\n",
      "stride:  (6, 2, 1)\n",
      "is_contiguous:  True\n"
     ]
    }
   ],
   "source": [
    "t10 = t6.view(2, 3, 2)\n",
    "print(\"t10: \", t10)\n",
    "print(\"shape: \", t10.shape)\n",
    "print(\"stride: \", t10.stride())\n",
    "print(\"is_contiguous: \", t10.is_contiguous())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t11:  tensor([[ 1,  5,  9],\n",
      "        [ 2,  6, 10],\n",
      "        [ 3,  7, 11],\n",
      "        [ 4,  8, 12]])\n",
      "shape:  torch.Size([4, 3])\n",
      "stride:  (1, 4)\n",
      "is_contiguous:  False\n"
     ]
    }
   ],
   "source": [
    "# Transpose operation usually breaks continuity\n",
    "t11 = t6.transpose(0, 1)\n",
    "print(\"t11: \", t11)\n",
    "print(\"shape: \", t11.shape)\n",
    "print(\"stride: \", t11.stride())\n",
    "print(\"is_contiguous: \", t11.is_contiguous())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t12:  tensor([[[ 1,  5,  9],\n",
      "         [ 2,  6, 10]],\n",
      "\n",
      "        [[ 3,  7, 11],\n",
      "         [ 4,  8, 12]]])\n",
      "shape:  torch.Size([2, 2, 3])\n",
      "stride:  (2, 1, 4)\n",
      "is_contiguous:  False\n",
      "Both t12 and t6 share the same storage.\n"
     ]
    }
   ],
   "source": [
    "# NOTICE THAT THE VIEW OPERATION ON t11 DID NOT RAISE AN ERROR AND HAS SUCCEEDED.\n",
    "# THIS MEANS VIEW CAN BE APPLIED EVEN IF THE TENSOR IS NOT CONTIGUOUS. IT JUST\n",
    "# NEEDS TO SATISFY CONTIGUITY LIKE CONDITION (NEED TO EXPLORE MORE ABOUT THIS).\n",
    "t12 = t11.view(2, 2, 3)\n",
    "print(\"t12: \", t12)\n",
    "print(\"shape: \", t12.shape)\n",
    "print(\"stride: \", t12.stride())\n",
    "print(\"is_contiguous: \", t12.is_contiguous())\n",
    "if t12.storage().data_ptr() == t6.storage().data_ptr():\n",
    "    print(\"Both t12 and t6 share the same storage.\")\n",
    "else:\n",
    "    print(\"Both t12 and t6 do not share the same storage.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [torch.transpose](https://pytorch.org/docs/stable/generated/torch.transpose.html#torch-transpose)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In 2D matrices, performing a transpose interchanges rows and columns. Old Rows\n",
    "# become the New columns and Old columns become the New rows.\n",
    "#\n",
    "# [0, 1, 2]\n",
    "# [3, 4, 5]\n",
    "#\n",
    "# when transposed changes to\n",
    "#\n",
    "# [0, 3]\n",
    "# [1, 4]\n",
    "# [2, 5]\n",
    "#\n",
    "# Transpose at higher dimensions also works in the same way but with a bit of\n",
    "# added complexity. Lets say we have a tensor of shape (2, 3, 4, 5) and want to\n",
    "# transpose the dimensions 0 and 3. The shape of the transposed matrix now\n",
    "# becomes (5, 3, 4, 2). The transpose operation follows the following logic:\n",
    "#\n",
    "# The order of the elements as we traverse the original tensor along dimension 0\n",
    "# is the same as the order of the elements as we traverse the transposed tensor\n",
    "# along dimension 3 and viceversa --> [I AM NOT 100% CERTAIN IF THIS IS CORRECT - \n",
    "# EXAMPLE BELOW]. \n",
    "# \n",
    "# It might be hard to intuitively understand what transpose does for higher dimensions. \n",
    "# The best way I understood is through the mathematical definition of transpose and \n",
    "# extending it to higher dimensions.\n",
    "#\n",
    "# The exact mathematical logic if a 4D tensor is transposed along dimensions 0 and 3 is:\n",
    "# OriginalTensor[ind0][ind1][ind2][ind3] = TransposedTensor[ind3][ind1][ind2][ind0].\n",
    "#\n",
    "# 1) Refer to understanding_tensors_part_1.ipynb (link to the notebook) to understand more\n",
    "#    about dimensions in a tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape:  torch.Size([3, 3, 3])\n",
      "t15:  tensor([[[ 0,  1,  2],\n",
      "         [ 3,  4,  5],\n",
      "         [ 6,  7,  8]],\n",
      "\n",
      "        [[ 9, 10, 11],\n",
      "         [12, 13, 14],\n",
      "         [15, 16, 17]],\n",
      "\n",
      "        [[18, 19, 20],\n",
      "         [21, 22, 23],\n",
      "         [24, 25, 26]]]) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "t13 = torch.arange(27).reshape(3, 3, 3)\n",
    "print(\"shape: \", t13.shape)\n",
    "print(\"t15: \", t13, \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape:  torch.Size([3, 3, 3])\n",
      "t16:  tensor([[[ 0,  9, 18],\n",
      "         [ 3, 12, 21],\n",
      "         [ 6, 15, 24]],\n",
      "\n",
      "        [[ 1, 10, 19],\n",
      "         [ 4, 13, 22],\n",
      "         [ 7, 16, 25]],\n",
      "\n",
      "        [[ 2, 11, 20],\n",
      "         [ 5, 14, 23],\n",
      "         [ 8, 17, 26]]])\n"
     ]
    }
   ],
   "source": [
    "# The order of elements as you traverse along dimension 0 in the original tensor (t13) is:\n",
    "# (0, 9, 18); (1, 10, 19); (2, 11, 20); (3, 12, 21); ... etc.\n",
    "#\n",
    "# The order of elements as you traverse along dimension 2 in the transposed tensor (t14) is:\n",
    "# (0, 9, 18); (1, 10, 19); (2, 11, 20); (3, 12, 21); ... etc.\n",
    "#\n",
    "# I am still a little confused here as to why the order of elements as you traverse along\n",
    "# dimension 2 in the transposed tensor (t14) is not:\n",
    "# (0, 9, 18); (3, 12, 21); (6, 15, 24): (1, 10, 19); .. etc.\n",
    "#\n",
    "# This is why I am not 100% certain about the logic I mentioned above. Either the transpose \n",
    "# logic (my verbal explanation) is wrong or the logic in determining the order of elements as \n",
    "# you traverse along some dimension is wrong. So, for now lets stick to the mathematical\n",
    "# definition of the transpose operation.\n",
    "t14 = t13.transpose(0, 2)\n",
    "print(\"shape: \", t14.shape)\n",
    "print(\"t16: \", t14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original_tensor_elem:  tensor(0)  : transposed_tensor_elem:  tensor(0)\n",
      "original_tensor_elem:  tensor(1)  : transposed_tensor_elem:  tensor(1)\n",
      "original_tensor_elem:  tensor(2)  : transposed_tensor_elem:  tensor(2)\n",
      "original_tensor_elem:  tensor(3)  : transposed_tensor_elem:  tensor(3)\n",
      "original_tensor_elem:  tensor(4)  : transposed_tensor_elem:  tensor(4)\n",
      "original_tensor_elem:  tensor(5)  : transposed_tensor_elem:  tensor(5)\n",
      "original_tensor_elem:  tensor(6)  : transposed_tensor_elem:  tensor(6)\n",
      "original_tensor_elem:  tensor(7)  : transposed_tensor_elem:  tensor(7)\n",
      "original_tensor_elem:  tensor(8)  : transposed_tensor_elem:  tensor(8)\n",
      "original_tensor_elem:  tensor(9)  : transposed_tensor_elem:  tensor(9)\n",
      "original_tensor_elem:  tensor(10)  : transposed_tensor_elem:  tensor(10)\n",
      "original_tensor_elem:  tensor(11)  : transposed_tensor_elem:  tensor(11)\n",
      "original_tensor_elem:  tensor(12)  : transposed_tensor_elem:  tensor(12)\n",
      "original_tensor_elem:  tensor(13)  : transposed_tensor_elem:  tensor(13)\n",
      "original_tensor_elem:  tensor(14)  : transposed_tensor_elem:  tensor(14)\n",
      "original_tensor_elem:  tensor(15)  : transposed_tensor_elem:  tensor(15)\n",
      "original_tensor_elem:  tensor(16)  : transposed_tensor_elem:  tensor(16)\n",
      "original_tensor_elem:  tensor(17)  : transposed_tensor_elem:  tensor(17)\n",
      "original_tensor_elem:  tensor(18)  : transposed_tensor_elem:  tensor(18)\n",
      "original_tensor_elem:  tensor(19)  : transposed_tensor_elem:  tensor(19)\n",
      "original_tensor_elem:  tensor(20)  : transposed_tensor_elem:  tensor(20)\n",
      "original_tensor_elem:  tensor(21)  : transposed_tensor_elem:  tensor(21)\n",
      "original_tensor_elem:  tensor(22)  : transposed_tensor_elem:  tensor(22)\n",
      "original_tensor_elem:  tensor(23)  : transposed_tensor_elem:  tensor(23)\n",
      "original_tensor_elem:  tensor(24)  : transposed_tensor_elem:  tensor(24)\n",
      "original_tensor_elem:  tensor(25)  : transposed_tensor_elem:  tensor(25)\n",
      "original_tensor_elem:  tensor(26)  : transposed_tensor_elem:  tensor(26)\n"
     ]
    }
   ],
   "source": [
    "# Printing the elements in the original tensor and the transpose of it.\n",
    "for ind0 in range(t13.size(0)):\n",
    "  for ind1 in range(t13.size(1)):\n",
    "    for ind2 in range(t13.size(2)):\n",
    "        print(\"original_tensor_elem: \", t13[ind0][ind1][ind2], \" : transposed_tensor_elem: \", t14[ind2][ind1][ind0])\n",
    "\n",
    "# Basically, like with the 2d matrices, the transpose logic follows that Mat[i][j] = Mat[j][i].\n",
    "# Visualizing the transpose for higher order tensors is not easy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [torch.tensor.repeat](https://pytorch.org/docs/stable/generated/torch.Tensor.repeat.html#torch-tensor-repeat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape:  torch.Size([2, 2, 3])\n",
      "t15: \n",
      " tensor([[[ 1,  2,  3],\n",
      "         [ 4,  5,  6]],\n",
      "\n",
      "        [[ 7,  8,  9],\n",
      "         [10, 11, 12]]])\n"
     ]
    }
   ],
   "source": [
    "# Repeat basically copies the tensor specified number of times along the specified dimension.\n",
    "t15 = torch.tensor(data=[[[1, 2, 3], [4, 5, 6]], [[7, 8, 9], [10, 11, 12]]], dtype=torch.int64)\n",
    "print(\"shape: \", t15.shape)\n",
    "print(\"t15: \\n\", t15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape:  torch.Size([4, 2, 3])\n",
      "t16: \n",
      " tensor([[[ 1,  2,  3],\n",
      "         [ 4,  5,  6]],\n",
      "\n",
      "        [[ 7,  8,  9],\n",
      "         [10, 11, 12]],\n",
      "\n",
      "        [[ 1,  2,  3],\n",
      "         [ 4,  5,  6]],\n",
      "\n",
      "        [[ 7,  8,  9],\n",
      "         [10, 11, 12]]])\n"
     ]
    }
   ],
   "source": [
    "# Note: Repeat does not change the number of dimensions of a tensor. It just copies the tensor along \n",
    "# the specified dimension. \n",
    "# The original two 2D tensors are repeated twice along the 0th dimension.\n",
    "# The arguments to repeat are the number of times to repeat the tensors along each dimension starting from the\n",
    "# 0th dimension. \n",
    "# 2 --> Repeat the tensor twice along the 0th dimension.\n",
    "# 1 --> Repeat the tensor once along the 1st dimension i.e., no change.\n",
    "# 1 --> Repeat the tensor once along the 2nd dimension i.e., no change.\n",
    "t16 = t15.repeat(2, 1, 1)\n",
    "print(\"shape: \", t16.shape)\n",
    "print(\"t16: \\n\", t16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape:  torch.Size([2, 4, 3])\n",
      "t17: \n",
      " tensor([[[ 1,  2,  3],\n",
      "         [ 4,  5,  6],\n",
      "         [ 1,  2,  3],\n",
      "         [ 4,  5,  6]],\n",
      "\n",
      "        [[ 7,  8,  9],\n",
      "         [10, 11, 12],\n",
      "         [ 7,  8,  9],\n",
      "         [10, 11, 12]]])\n"
     ]
    }
   ],
   "source": [
    "# The original four 1D tensors (two each in one 2D tensor) are repeated twice along the 1st dimension. \n",
    "t17 = t15.repeat(1, 2, 1)\n",
    "print(\"shape: \", t17.shape)\n",
    "print(\"t17: \\n\", t17)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape:  torch.Size([4, 4, 3])\n",
      "t18: \n",
      " tensor([[[ 1,  2,  3],\n",
      "         [ 4,  5,  6],\n",
      "         [ 1,  2,  3],\n",
      "         [ 4,  5,  6]],\n",
      "\n",
      "        [[ 7,  8,  9],\n",
      "         [10, 11, 12],\n",
      "         [ 7,  8,  9],\n",
      "         [10, 11, 12]],\n",
      "\n",
      "        [[ 1,  2,  3],\n",
      "         [ 4,  5,  6],\n",
      "         [ 1,  2,  3],\n",
      "         [ 4,  5,  6]],\n",
      "\n",
      "        [[ 7,  8,  9],\n",
      "         [10, 11, 12],\n",
      "         [ 7,  8,  9],\n",
      "         [10, 11, 12]]])\n"
     ]
    }
   ],
   "source": [
    "# Now, lets try to figure out how repeat works if it is given positive value (>1) in multiple dimensions.\n",
    "# It is equivalent to first applying the repeat operation in the 0th dimension and then applying the repeat\n",
    "# operation in the 1st dimension and so on.\n",
    "t18 = t15.repeat(2, 2, 1)\n",
    "print(\"shape: \", t18.shape)\n",
    "print(\"t18: \\n\", t18)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".pytorch_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
