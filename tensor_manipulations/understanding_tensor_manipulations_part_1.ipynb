{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In this notebook, you learn:\n",
    "#\n",
    "# 1) What does torch.unsqueeze do?\n",
    "# 2) What does torch.nn.functional.pad do?\n",
    "# 3) How to slice a tensor?\n",
    "# 4) What does torch.unbind do?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [torch.unsqueeze](https://www.google.com/url?q=https%3A%2F%2Fpytorch.org%2Fdocs%2Fstable%2Fgenerated%2Ftorch.unsqueeze.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# unsqueeze basically adds a dimension at the given position. Lets think of a tensor as a container of \n",
    "# smaller tensors. If dim = 2 is used with unsqueeze, it means we go inside 2 containers and add a \n",
    "# container for all the tensors after traversing 2 steps i.e., we traverse 0, 1 dimensions and add an \n",
    "# extra dimension to every tensor we encounter after traversing 0, 1 dimensions.    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape:  torch.Size([2, 3, 3])\n",
      "t1:  tensor([[[ 1,  2,  3],\n",
      "         [ 4,  5,  6],\n",
      "         [ 7,  8,  9]],\n",
      "\n",
      "        [[10, 11, 12],\n",
      "         [13, 14, 15],\n",
      "         [16, 17, 18]]])\n"
     ]
    }
   ],
   "source": [
    "t1 = torch.tensor(data=[[[1, 2, 3], [4, 5, 6], [7, 8, 9]], [[10, 11, 12], [13, 14, 15], [16, 17, 18]]])\n",
    "print(\"shape: \", t1.shape)\n",
    "print(\"t1: \", t1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape:  torch.Size([1, 2, 3, 3])\n",
      "t2:  tensor([[[[ 1,  2,  3],\n",
      "          [ 4,  5,  6],\n",
      "          [ 7,  8,  9]],\n",
      "\n",
      "         [[10, 11, 12],\n",
      "          [13, 14, 15],\n",
      "          [16, 17, 18]]]])\n"
     ]
    }
   ],
   "source": [
    "# Creates a new dimension (which acts as dimension 0) and places the original tensor 't1' along this dimension.\n",
    "# To summarize, it just adds an additional container on top of our tensor 't1' to create 't2'.\n",
    "t2 = torch.unsqueeze(input=t1, dim = 0)\n",
    "print(\"shape: \", t2.shape)\n",
    "print(\"t2: \", t2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape:  torch.Size([2, 1, 3, 3])\n",
      "t3:  tensor([[[[ 1,  2,  3],\n",
      "          [ 4,  5,  6],\n",
      "          [ 7,  8,  9]]],\n",
      "\n",
      "\n",
      "        [[[10, 11, 12],\n",
      "          [13, 14, 15],\n",
      "          [16, 17, 18]]]])\n"
     ]
    }
   ],
   "source": [
    "# Traverse 1 level inside (1 container) 't1'. We get the 2 '2D' tensors [[1, 2, 3], [4, 5, 6], [7, 8, 9]] and \n",
    "# [[10, 11, 12], [13, 14, 15], [16, 17, 18]]. Each of these two tensors of shape (3, 3) are put inside another \n",
    "# container to create new tensors of shape (1, 3, 3). So, finally we get a '4D' tensor containing 2 '3D' tensors. \n",
    "t3 = torch.unsqueeze(input=t1, dim=1)\n",
    "print(\"shape: \", t3.shape)\n",
    "print(\"t3: \", t3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape:  torch.Size([2, 3, 1, 3])\n",
      "t4:  tensor([[[[ 1,  2,  3]],\n",
      "\n",
      "         [[ 4,  5,  6]],\n",
      "\n",
      "         [[ 7,  8,  9]]],\n",
      "\n",
      "\n",
      "        [[[10, 11, 12]],\n",
      "\n",
      "         [[13, 14, 15]],\n",
      "\n",
      "         [[16, 17, 18]]]])\n"
     ]
    }
   ],
   "source": [
    "# Traverse 2 levels inside (1 container) 't1'. We get the six '1D' tensors [1, 2, 3], [4, 5, 6], [7, 8, 9] and \n",
    "# [10, 11, 12], [13, 14, 15], [16, 17, 18]. Each of these six tensors of shape (3,) are put inside another \n",
    "# container to create new tensors of shape (1, 3). So, finally we get a '4D' tensor containing 2 '3D' tensors. \n",
    "t4 = torch.unsqueeze(input=t1, dim=2)\n",
    "print(\"shape: \", t4.shape)\n",
    "print(\"t4: \", t4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[ 1],\n",
      "          [ 2],\n",
      "          [ 3]],\n",
      "\n",
      "         [[ 4],\n",
      "          [ 5],\n",
      "          [ 6]],\n",
      "\n",
      "         [[ 7],\n",
      "          [ 8],\n",
      "          [ 9]]],\n",
      "\n",
      "\n",
      "        [[[10],\n",
      "          [11],\n",
      "          [12]],\n",
      "\n",
      "         [[13],\n",
      "          [14],\n",
      "          [15]],\n",
      "\n",
      "         [[16],\n",
      "          [17],\n",
      "          [18]]]]) torch.Size([2, 3, 3, 1])\n"
     ]
    }
   ],
   "source": [
    "# Traverse 3 levels inside (1 container) 't1'. We get the 18 individual numbers 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, \n",
    "# 11, 12, 13, 14, 15, 16, 17, 18. Each of these 18 numbers are put inside a container to create new tensors of shape (1,). \n",
    "t5 = torch.unsqueeze(input=t1, dim=3)\n",
    "print(t5, t5.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [torch.squeeze](https://pytorch.org/docs/main/generated/torch.squeeze.html#torch-squeeze)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO BE ADDED"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [torch.nn.functional.pad](https://pytorch.org/docs/stable/generated/torch.nn.functional.pad.html#torch.nn.functional.pad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pads the given 'input' tensor with the provided 'value'.\n",
    "# argument pad=(3, 2) means pads 3 values at the start and 2 values at the end for the tensors in the last dimension.\n",
    "# So, [10, 20] tensor when padded using pad=(3, 2) turns into [2.5, 2.5, 2.5, 10.0, 20.0, 2.5, 2.5].\n",
    "# Size in the last dimension increase by 3 + 2 = 5.\n",
    "#\n",
    "# In general, then pad has the following form:\n",
    "#\n",
    "# (padding_left, padding_right) to pad only the last dimension of the input tensor. \n",
    "# (padding_left, padding_right, padding_top, padding_bottom) to pad the last 2 dimensions of the input tensor.\n",
    "# (padding_left, padding_right, padding_top, padding_bottom, padding_front, padding_back) to pad the last 3 \n",
    "#       dimensions of the input tensor.\n",
    "# \n",
    "# Similary extend the logic to all higher dimensions.\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 1.,  2.,  3.],\n",
      "         [ 4.,  5.,  6.]],\n",
      "\n",
      "        [[ 7.,  8.,  9.],\n",
      "         [10., 11., 12.]],\n",
      "\n",
      "        [[13., 14., 15.],\n",
      "         [16., 17., 18.]]], dtype=torch.float64) torch.Size([3, 2, 3])\n"
     ]
    }
   ],
   "source": [
    "t6 = torch.tensor([[[1, 2, 3], [4, 5, 6]], [[7, 8, 9], [10, 11, 12]], [[13, 14, 15], [16, 17, 18]]], dtype=float)\n",
    "print(t6, t6.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 2.5000,  2.5000,  2.5000,  1.0000,  2.0000,  3.0000,  2.5000,\n",
      "           2.5000],\n",
      "         [ 2.5000,  2.5000,  2.5000,  4.0000,  5.0000,  6.0000,  2.5000,\n",
      "           2.5000]],\n",
      "\n",
      "        [[ 2.5000,  2.5000,  2.5000,  7.0000,  8.0000,  9.0000,  2.5000,\n",
      "           2.5000],\n",
      "         [ 2.5000,  2.5000,  2.5000, 10.0000, 11.0000, 12.0000,  2.5000,\n",
      "           2.5000]],\n",
      "\n",
      "        [[ 2.5000,  2.5000,  2.5000, 13.0000, 14.0000, 15.0000,  2.5000,\n",
      "           2.5000],\n",
      "         [ 2.5000,  2.5000,  2.5000, 16.0000, 17.0000, 18.0000,  2.5000,\n",
      "           2.5000]]], dtype=torch.float64) torch.Size([3, 2, 8])\n"
     ]
    }
   ],
   "source": [
    "# Notice that it added 3 values at the start and 2 values at the end for the tensors in the last dimension.\n",
    "t7 = torch.nn.functional.pad(input=t6, pad=(3, 2), mode=\"constant\", value=2.5)\n",
    "print(t7, t7.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 2.5000,  2.5000,  2.5000,  2.5000,  2.5000,  2.5000,  2.5000,\n",
      "           2.5000],\n",
      "         [ 2.5000,  2.5000,  2.5000,  1.0000,  2.0000,  3.0000,  2.5000,\n",
      "           2.5000],\n",
      "         [ 2.5000,  2.5000,  2.5000,  4.0000,  5.0000,  6.0000,  2.5000,\n",
      "           2.5000],\n",
      "         [ 2.5000,  2.5000,  2.5000,  2.5000,  2.5000,  2.5000,  2.5000,\n",
      "           2.5000]],\n",
      "\n",
      "        [[ 2.5000,  2.5000,  2.5000,  2.5000,  2.5000,  2.5000,  2.5000,\n",
      "           2.5000],\n",
      "         [ 2.5000,  2.5000,  2.5000,  7.0000,  8.0000,  9.0000,  2.5000,\n",
      "           2.5000],\n",
      "         [ 2.5000,  2.5000,  2.5000, 10.0000, 11.0000, 12.0000,  2.5000,\n",
      "           2.5000],\n",
      "         [ 2.5000,  2.5000,  2.5000,  2.5000,  2.5000,  2.5000,  2.5000,\n",
      "           2.5000]],\n",
      "\n",
      "        [[ 2.5000,  2.5000,  2.5000,  2.5000,  2.5000,  2.5000,  2.5000,\n",
      "           2.5000],\n",
      "         [ 2.5000,  2.5000,  2.5000, 13.0000, 14.0000, 15.0000,  2.5000,\n",
      "           2.5000],\n",
      "         [ 2.5000,  2.5000,  2.5000, 16.0000, 17.0000, 18.0000,  2.5000,\n",
      "           2.5000],\n",
      "         [ 2.5000,  2.5000,  2.5000,  2.5000,  2.5000,  2.5000,  2.5000,\n",
      "           2.5000]]], dtype=torch.float64) torch.Size([3, 4, 8])\n"
     ]
    }
   ],
   "source": [
    "# Notice that it added two new 1D tensors for every 2D tensor. \n",
    "t8 = torch.nn.functional.pad(input=t6, pad=(3, 2, 1, 1), mode=\"constant\", value=2.5)\n",
    "print(t8, t8.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Slicing in PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 5])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[ 1,  2,  3,  4,  5],\n",
       "        [ 6,  7,  8,  9, 10],\n",
       "        [11, 12, 13, 14, 15],\n",
       "        [16, 17, 18, 19, 20]])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Lets start with a simple 2D tensor and then move to higher dimensions.\n",
    "t9 = torch.arange(start=1, end=21).reshape(4, 5)\n",
    "print(t9.shape)\n",
    "t9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Slicing is basically indexing into the tensor to retrieve parts of the tensor. This is similar to indexing in arrays.\n",
    "# The tensor 't9' has 2 dimensions and so we can use 2 pairs of start-end tuples to retrieve the slices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 3])\n",
      "tensor([[ 2,  3,  4],\n",
      "        [ 7,  8,  9],\n",
      "        [12, 13, 14],\n",
      "        [17, 18, 19]])\n"
     ]
    }
   ],
   "source": [
    "# dimension 0 --> : --> This means start=0 and end=4.\n",
    "#                       This retrieves all the 4 tensors along dimension 0 i.e., all 4 rows.\n",
    "# dimension 1 --> 1:4 --> This means start=1 and end=4\n",
    "#                         From each of the 4 tensors obtained after 0th step, this retrives the elements between indices\n",
    "#                         1 and 3 (inclusive).\n",
    "t10 = t9[:, 1:4]\n",
    "print(t10.shape)\n",
    "print(t10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([2, 3, 4])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# dimension 0 --> 0 --> This means just get the 0th sub-tensor along dimension 0. Since we specified only 1 number, this\n",
    "#                       will be removed and the resultant tensor will have 1 less dimension.\n",
    "# dimension 1 --> 1:4 --> This again means start=1 and end=4.\n",
    "#                         We only have 1 tensor obtained from 0th step. So, this just retrieves all the elements between\n",
    "#                         indices 1 and 3 (inclusive) from the tensor obtained in 0th step.\n",
    "t11 = t9[0, 1:4]\n",
    "print(t11.shape)\n",
    "t11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 0])\n",
      "tensor([], size=(4, 0), dtype=torch.int64)\n"
     ]
    }
   ],
   "source": [
    "# dimension 0 --> : --> This means start=0 and end=4.\n",
    "#                       This retrieves all the 4 tensors along dimension 0 i.e., all 4 rows.\n",
    "# dimension 1 --> 2:2 --> This means retrieve no elements along this dimension. This will result in an empty tensor.\n",
    "#\n",
    "# Even though we get an empty tensor, the shape is still preserved.\n",
    "t12 = t9[:, 2:2]\n",
    "print(t12.shape)\n",
    "print(t12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 3])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[ 3,  4,  5],\n",
       "        [ 8,  9, 10],\n",
       "        [13, 14, 15]])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# dimension 0 --> 0:3 --> This means start=0 and end=3. This will retrieve all the sub-tensors between indices 0 and \n",
    "#                         2 (inclusive).\n",
    "# dimension 1 --> 2:5 --> This means start=2 and end=5.\n",
    "#                         From each of the tensors retrieved in step 0, it retrieves the elements between indices 2\n",
    "#                         4 (inclusive).\n",
    "t13 = t9[0:3, 2:5]\n",
    "print(t13.shape)\n",
    "t13"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now, let's just try the same on a 4D tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 3, 3, 3])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[[ 1,  2,  3],\n",
       "          [ 4,  5,  6],\n",
       "          [ 7,  8,  9]],\n",
       "\n",
       "         [[10, 11, 12],\n",
       "          [13, 14, 15],\n",
       "          [16, 17, 18]],\n",
       "\n",
       "         [[19, 20, 21],\n",
       "          [22, 23, 24],\n",
       "          [25, 26, 27]]],\n",
       "\n",
       "\n",
       "        [[[28, 29, 30],\n",
       "          [31, 32, 33],\n",
       "          [34, 35, 36]],\n",
       "\n",
       "         [[37, 38, 39],\n",
       "          [40, 41, 42],\n",
       "          [43, 44, 45]],\n",
       "\n",
       "         [[46, 47, 48],\n",
       "          [49, 50, 51],\n",
       "          [52, 53, 54]]],\n",
       "\n",
       "\n",
       "        [[[55, 56, 57],\n",
       "          [58, 59, 60],\n",
       "          [61, 62, 63]],\n",
       "\n",
       "         [[64, 65, 66],\n",
       "          [67, 68, 69],\n",
       "          [70, 71, 72]],\n",
       "\n",
       "         [[73, 74, 75],\n",
       "          [76, 77, 78],\n",
       "          [79, 80, 81]]]])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t14 = torch.arange(start=1, end=82).reshape(3, 3, 3, 3)\n",
    "print(t14.shape)\n",
    "t14"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The tensor 't14' has 4 dimensions and so we can use 4 pairs of start-end tuples to retrieve the slices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 2, 1])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[ 6],\n",
       "         [15]],\n",
       "\n",
       "        [[33],\n",
       "         [42]]])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's evaluate the slicing from left to right.\n",
    "# 0:2 --> Retrieves '2' 3D sub-tensors along dimension 0.\n",
    "#\n",
    "# 0:2 --> Retrieves '2' 2D sub-tensors from each of the two 3D sub-tensors retrieved in step 0.\n",
    "#\n",
    "# 1:2 --> Retrieves '1' 1D sub-tensor from each of the four 2D sub-tensor obtained after step 1.\n",
    "#\n",
    "# 2   --> Retrieves just the 2nd element in each of the '4' 1D sub-tensors obtained after step 2.\n",
    "t15 = t14[0:2, 0:2, 1:2, 2]\n",
    "print(t15.shape)\n",
    "t15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can keep using the same logic no matter what the dimension of the initial tensor is."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [torch.unbind](https://pytorch.org/docs/stable/generated/torch.unbind.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Honestly, the explanation on the Pytorch website doesn't make any sense. However, apprently 'unbind' is a\n",
    "# specific way of slicing into the tensor. So, it would be helpful if you understood how slicing works from\n",
    "# the above function.\n",
    "#\n",
    "# 'unbind' is the same as applying 'slicing' multiple times along the specified dimension. It is easier to\n",
    "# understand from the runs below instead of trying to explain it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 4, 5])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[ 1,  2,  3,  4,  5],\n",
       "         [ 6,  7,  8,  9, 10],\n",
       "         [11, 12, 13, 14, 15],\n",
       "         [16, 17, 18, 19, 20]],\n",
       "\n",
       "        [[21, 22, 23, 24, 25],\n",
       "         [26, 27, 28, 29, 30],\n",
       "         [31, 32, 33, 34, 35],\n",
       "         [36, 37, 38, 39, 40]],\n",
       "\n",
       "        [[41, 42, 43, 44, 45],\n",
       "         [46, 47, 48, 49, 50],\n",
       "         [51, 52, 53, 54, 55],\n",
       "         [56, 57, 58, 59, 60]]])"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t16 = torch.arange(start=1, end=61).reshape(3, 4, 5)\n",
    "print(t16.shape)\n",
    "t16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1,  2,  3,  4,  5],\n",
      "        [ 6,  7,  8,  9, 10],\n",
      "        [11, 12, 13, 14, 15],\n",
      "        [16, 17, 18, 19, 20]])\n",
      "tensor([[21, 22, 23, 24, 25],\n",
      "        [26, 27, 28, 29, 30],\n",
      "        [31, 32, 33, 34, 35],\n",
      "        [36, 37, 38, 39, 40]])\n",
      "tensor([[41, 42, 43, 44, 45],\n",
      "        [46, 47, 48, 49, 50],\n",
      "        [51, 52, 53, 54, 55],\n",
      "        [56, 57, 58, 59, 60]])\n"
     ]
    }
   ],
   "source": [
    "print(t16[0, :, :])\n",
    "print(t16[1, :, :])\n",
    "print(t16[2, :, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor([[ 1,  2,  3,  4,  5],\n",
       "         [ 6,  7,  8,  9, 10],\n",
       "         [11, 12, 13, 14, 15],\n",
       "         [16, 17, 18, 19, 20]]),\n",
       " tensor([[21, 22, 23, 24, 25],\n",
       "         [26, 27, 28, 29, 30],\n",
       "         [31, 32, 33, 34, 35],\n",
       "         [36, 37, 38, 39, 40]]),\n",
       " tensor([[41, 42, 43, 44, 45],\n",
       "         [46, 47, 48, 49, 50],\n",
       "         [51, 52, 53, 54, 55],\n",
       "         [56, 57, 58, 59, 60]]))"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l1 = torch.unbind(input=t16, dim=0)\n",
    "print(len(l1))\n",
    "l1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1,  2,  3,  4,  5],\n",
      "        [21, 22, 23, 24, 25],\n",
      "        [41, 42, 43, 44, 45]])\n",
      "tensor([[ 6,  7,  8,  9, 10],\n",
      "        [26, 27, 28, 29, 30],\n",
      "        [46, 47, 48, 49, 50]])\n",
      "tensor([[11, 12, 13, 14, 15],\n",
      "        [31, 32, 33, 34, 35],\n",
      "        [51, 52, 53, 54, 55]])\n",
      "tensor([[16, 17, 18, 19, 20],\n",
      "        [36, 37, 38, 39, 40],\n",
      "        [56, 57, 58, 59, 60]])\n"
     ]
    }
   ],
   "source": [
    "print(t16[:, 0, :])\n",
    "print(t16[:, 1, :])\n",
    "print(t16[:, 2, :])\n",
    "print(t16[:, 3, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor([[ 1,  2,  3,  4,  5],\n",
       "         [21, 22, 23, 24, 25],\n",
       "         [41, 42, 43, 44, 45]]),\n",
       " tensor([[ 6,  7,  8,  9, 10],\n",
       "         [26, 27, 28, 29, 30],\n",
       "         [46, 47, 48, 49, 50]]),\n",
       " tensor([[11, 12, 13, 14, 15],\n",
       "         [31, 32, 33, 34, 35],\n",
       "         [51, 52, 53, 54, 55]]),\n",
       " tensor([[16, 17, 18, 19, 20],\n",
       "         [36, 37, 38, 39, 40],\n",
       "         [56, 57, 58, 59, 60]]))"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l2 = torch.unbind(input=t16, dim=1)\n",
    "print(len(l2))\n",
    "l2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1,  6, 11, 16],\n",
      "        [21, 26, 31, 36],\n",
      "        [41, 46, 51, 56]])\n",
      "tensor([[ 2,  7, 12, 17],\n",
      "        [22, 27, 32, 37],\n",
      "        [42, 47, 52, 57]])\n",
      "tensor([[ 3,  8, 13, 18],\n",
      "        [23, 28, 33, 38],\n",
      "        [43, 48, 53, 58]])\n",
      "tensor([[ 4,  9, 14, 19],\n",
      "        [24, 29, 34, 39],\n",
      "        [44, 49, 54, 59]])\n",
      "tensor([[ 5, 10, 15, 20],\n",
      "        [25, 30, 35, 40],\n",
      "        [45, 50, 55, 60]])\n"
     ]
    }
   ],
   "source": [
    "print(t16[:, :, 0])\n",
    "print(t16[:, :, 1])\n",
    "print(t16[:, :, 2])\n",
    "print(t16[:, :, 3])\n",
    "print(t16[:, :, 4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor([[ 1,  6, 11, 16],\n",
       "         [21, 26, 31, 36],\n",
       "         [41, 46, 51, 56]]),\n",
       " tensor([[ 2,  7, 12, 17],\n",
       "         [22, 27, 32, 37],\n",
       "         [42, 47, 52, 57]]),\n",
       " tensor([[ 3,  8, 13, 18],\n",
       "         [23, 28, 33, 38],\n",
       "         [43, 48, 53, 58]]),\n",
       " tensor([[ 4,  9, 14, 19],\n",
       "         [24, 29, 34, 39],\n",
       "         [44, 49, 54, 59]]),\n",
       " tensor([[ 5, 10, 15, 20],\n",
       "         [25, 30, 35, 40],\n",
       "         [45, 50, 55, 60]]))"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l3 = torch.unbind(input=t16, dim=2)\n",
    "print(len(l3))\n",
    "l3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".pytorch_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
