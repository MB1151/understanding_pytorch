{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In this notebook, you learn:\n",
    "#\n",
    "# 1) What is stride of a tensor?\n",
    "# 2) How to access underlying tensor storage in PyTorch?\n",
    "# 3) What is the difference between tensor.data_ptr() and tensor.storage().data_ptr()?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import ctypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resources to go through before contuning further in this notebook:\n",
    "#\n",
    "# 1) https://martinlwx.github.io/en/how-to-reprensent-a-tensor-or-ndarray/\n",
    "#       -- This blog post explains how a tensor is represented in memory and how are the values accessed.\n",
    "# 2) https://discuss.pytorch.org/t/contigious-vs-non-contigious-tensor/30107/2\n",
    "#      -- This forum post explains what is a contiguous tensor and how to make a tensor contiguous."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [torch.tensor](https://pytorch.org/docs/stable/tensors.html#torch-tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[ 0,  1],\n",
      "          [ 2,  3],\n",
      "          [ 4,  5]],\n",
      "\n",
      "         [[ 6,  7],\n",
      "          [ 8,  9],\n",
      "          [10, 11]]],\n",
      "\n",
      "\n",
      "        [[[12, 13],\n",
      "          [14, 15],\n",
      "          [16, 17]],\n",
      "\n",
      "         [[18, 19],\n",
      "          [20, 21],\n",
      "          [22, 23]]]])\n",
      "shape:  torch.Size([2, 2, 3, 2])\n",
      "stride:  (12, 6, 2, 1)\n"
     ]
    }
   ],
   "source": [
    "t1 = torch.tensor(data=[[[[0, 1], [2, 3], [4, 5]], [[6, 7], [8, 9], [10, 11]]], [[[12, 13], [14, 15], [16, 17]], [[18, 19], [20, 21], [22, 23]]]], dtype=torch.int64)\n",
    "print(t1)\n",
    "print(\"shape: \", t1.shape)\n",
    "# Stride is a tuple of integers each of which represents the number of elements in the storage that need to be jumped over \n",
    "# to obtain the next element along each dimension.\n",
    "# To get to the next element:\n",
    "# 1) In dimension 0, we need to jump 12 elements.\n",
    "#       -- Imagine you are at 0. 12 is the next element along dimension 0. So, you need to jump 12 elements to get to the next element.\n",
    "# 2) In dimension 1, we need to jump 6 elements.\n",
    "#       -- Imagine you are at 0. 6 is the next element along dimension 1. So, you need to jump 6 elements to get to the next element.\n",
    "# 3) In dimension 2, we need to jump 2 elements.\n",
    "#       -- Imagine you are at 0. 2 is the next element along dimension 2. So, you need to jump 2 elements to get to the next element.\n",
    "# 4) In dimension 3, we need to jump 1 element.\n",
    "#       -- Imagine you are at 0. 1 is the next element along dimension 3. So, you need to jump 1 element to get to the next element.\n",
    "#\n",
    "# So, the stride is (12, 6, 2, 1).\n",
    "print(\"stride: \", t1.stride())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0,  1,  2,  3],\n",
      "        [ 4,  5,  6,  7],\n",
      "        [ 8,  9, 10, 11]])\n",
      "shape:  torch.Size([3, 4])\n",
      "stride:  (4, 1)\n",
      "is_contiguous:  True\n",
      "storage:   0\n",
      " 1\n",
      " 2\n",
      " 3\n",
      " 4\n",
      " 5\n",
      " 6\n",
      " 7\n",
      " 8\n",
      " 9\n",
      " 10\n",
      " 11\n",
      "[torch.storage.TypedStorage(dtype=torch.int64, device=cpu) of size 12]\n"
     ]
    }
   ],
   "source": [
    "# Contrary to the popular belief, a tensor is not a multi-dimensional array. It is a view \n",
    "# of a storage. A storage is a contiguous block of memory. A tensor is a view of this storage.\n",
    "#\n",
    "# Tensor can be thought of as an object that saves the information which tells how to view \n",
    "# the data in the storage. So, the same storage can be viewed in multiple ways and hence \n",
    "# can be represented differently with multiple tensors. \n",
    "#\n",
    "# Example: An array [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12] of size 12 can either be viewed \n",
    "# as a tensor of shape (2, 6) or a tensor of shape (3, 4). \n",
    "#\n",
    "# However, the operations on one tensor affect the other tensor if they both share the same \n",
    "# storage. We will show an example of this below.\n",
    "t2 = torch.tensor(data=[[0, 1, 2, 3], [4, 5, 6, 7], [8, 9, 10, 11]], dtype=torch.int64)\n",
    "print(t2)\n",
    "print(\"shape: \", t2.shape)\n",
    "print(\"stride: \", t2.stride())\n",
    "# Says if the elements in the tensor elements are stored contiguously in memory.\n",
    "# Note: 'is_contiguous=True' means the elements in the tensor are stored sequentially (continuously)\n",
    "# within the storage. non-contiguous means the elements in the tensor are not stored\n",
    "# sequentially within the storage. It has nothing to do with the whether the storage is a \n",
    "# continuous block of memory or not. However, I assume the underlying storage is not fragmented \n",
    "# but always a continuous block of memory. \n",
    "print(\"is_contiguous: \", t2.is_contiguous())\n",
    "print(\"storage: \", t2.storage())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0,  4,  8],\n",
      "        [ 1,  5,  9],\n",
      "        [ 2,  6, 10],\n",
      "        [ 3,  7, 11]])\n",
      "shape:  torch.Size([4, 3])\n",
      "stride:  (1, 4)\n",
      "is_continuous:  False\n",
      "storage:   0\n",
      " 1\n",
      " 2\n",
      " 3\n",
      " 4\n",
      " 5\n",
      " 6\n",
      " 7\n",
      " 8\n",
      " 9\n",
      " 10\n",
      " 11\n",
      "[torch.storage.TypedStorage(dtype=torch.int64, device=cpu) of size 12]\n"
     ]
    }
   ],
   "source": [
    "# You might be wondering when will a tensor be non-contiguous. It can happen when you do operations\n",
    "# like slicing, transposing, etc. Lets see an example of this.\n",
    "# This is the same as transpose operation in matrices. Rows become columns and columns become rows.\n",
    "# The underlying storage is not changed. Only the view of the storage is changed which updates the\n",
    "# stride of the tensor.\n",
    "t3 = t2.transpose(0, 1)\n",
    "print(t3)\n",
    "print(\"shape: \", t3.shape)\n",
    "print(\"stride: \", t3.stride())\n",
    "# Transpose operation often breaks contiguity.\n",
    "print(\"is_continuous: \", t3.is_contiguous())\n",
    "print(\"storage: \", t3.storage())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Both t2 and t3 share the same storage.\n"
     ]
    }
   ],
   "source": [
    "# The underlying storage for both t2 and t3 is the same. This is because the transposed tensor t3 is a \n",
    "# view of the original tensor t2. There is no need to reshape t2 to obtain t3. It is enough just to use\n",
    "# different strides to have different views of the same storage.\n",
    "# We will look at how 'view' and 'reshape' work in a different notebook.\n",
    "#\n",
    "# tensor.storage().data_ptr() returns the memory address of the first element of the underlying storage.\n",
    "if t2.storage().data_ptr() == t3.storage().data_ptr():\n",
    "    print(\"Both t2 and t3 share the same storage.\")\n",
    "else:\n",
    "    print(\"Both t2 and t3 do not share the same storage.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dtype:  torch.int64\n",
      "tensor pointer type:  <class 'int'>\n",
      "stride:  (12, 6, 2, 1)\n",
      "storage:   0\n",
      " 1\n",
      " 2\n",
      " 3\n",
      " 4\n",
      " 5\n",
      " 6\n",
      " 7\n",
      " 8\n",
      " 9\n",
      " 10\n",
      " 11\n",
      " 12\n",
      " 13\n",
      " 14\n",
      " 15\n",
      " 16\n",
      " 17\n",
      " 18\n",
      " 19\n",
      " 20\n",
      " 21\n",
      " 22\n",
      " 23\n",
      "[torch.storage.TypedStorage(dtype=torch.int64, device=cpu) of size 24]\n",
      "element_size:  8\n"
     ]
    }
   ],
   "source": [
    "# Lets go back to our original 3D tensor t1 and see some of these properties.\n",
    "# Lets try to access the underlying storage of a tensor.\n",
    "print(\"dtype: \", t1.dtype)\n",
    "# It is a pointer to the first element of the tensor (not the storage - There is a difference).\n",
    "t1_data_ptr = t1.data_ptr()\n",
    "print(\"tensor pointer type: \", type(t1_data_ptr))\n",
    "print(\"stride: \", t1.stride())\n",
    "# The storage is a low level object representing a 1D array of bytes. It is a contiguous \n",
    "# block of memory. Pytorch interprets these bytes as a specific data type.\n",
    "t1_storage = t1.storage()\n",
    "print(\"storage: \", t1_storage)\n",
    "# Gives the size in bytes of an individual element in the storage. torch.int64 is 8 bytes (64 bits).\n",
    "element_size = t1_storage.element_size()\n",
    "print(\"element_size: \", element_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "element at index 0:  0\n",
      "element at index 4:  4\n"
     ]
    }
   ],
   "source": [
    "# Reading the elements from the underlying storage using pointers (address of the elements). \n",
    "# Handling pointers in Python is generally not recommended.\n",
    "elem_at_index_0 = ctypes.cast(t1_data_ptr, ctypes.POINTER(ctypes.c_int64))[0]\n",
    "print(\"element at index 0: \", elem_at_index_0)\n",
    "index = 4\n",
    "offset = index * element_size\n",
    "element_data_ptr = t1_data_ptr + offset\n",
    "element_at_index_4 = ctypes.cast(element_data_ptr, ctypes.POINTER(ctypes.c_int))[0]\n",
    "print(\"element at index 4: \", element_at_index_4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t4:  tensor([[0, 1, 2],\n",
      "        [3, 4, 5]])\n",
      "shape:  torch.Size([2, 3])\n",
      "stride:  (3, 1)\n"
     ]
    }
   ],
   "source": [
    "# There is an important difference between t1.data_ptr() and t1.storage.data_ptr().\n",
    "# \n",
    "# t1.data_ptr() returns the memory address of the first element of the tensor.\n",
    "# t1.storage().data_ptr() returns the memory address of the first element of the underlying storage.\n",
    "# \n",
    "# The underlying storage is a contiguous block of memory. The tensor is a view of this storage.\n",
    "# Sometimes, it is possible that the first element in the underlying storage is not the same\n",
    "# as the first element of the tensor (a view of the storage).\n",
    "\n",
    "t4 = torch.tensor(data=[[0, 1, 2], [3, 4, 5]], dtype=torch.int64)\n",
    "print(\"t4: \", t4)\n",
    "print(\"shape: \", t4.shape)\n",
    "print(\"stride: \", t4.stride())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t5:  tensor([1, 4])\n",
      "shape:  torch.Size([2])\n",
      "stride:  (3,)\n"
     ]
    }
   ],
   "source": [
    "t5 = t4[:, 1]\n",
    "print(\"t5: \", t5)\n",
    "print(\"shape: \", t5.shape)\n",
    "print(\"stride: \", t5.stride())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t4 data pointer:  94770823539648\n",
      "t5 data pointer:  94770823539656\n"
     ]
    }
   ],
   "source": [
    "# Notice that the data_ptr addresses are different for t4 and t5. This is because the first element \n",
    "# of t4 '0' and the first element of t5 '1' have different starting addresses. \n",
    "t4_data_ptr = t4.data_ptr()\n",
    "print(\"t4 data pointer: \", t4_data_ptr)\n",
    "t5_data_ptr = t5.data_ptr()\n",
    "print(\"t5 data pointer: \", t5_data_ptr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t4 storage data pointer:  94770823539648\n",
      "t5 storage data pointer:  94770823539648\n"
     ]
    }
   ],
   "source": [
    "# Notice that the storage_data_ptr addresses are same for both t4 and t5. This is because the underlying\n",
    "# storage is the same for both t4 and t5.\n",
    "t4_storage_data_ptr = t4.storage().data_ptr()\n",
    "print(\"t4 storage data pointer: \", t4_storage_data_ptr)\n",
    "t5_storage_data_ptr = t5.storage().data_ptr()\n",
    "print(\"t5 storage data pointer: \", t5_storage_data_ptr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".pytorch_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
