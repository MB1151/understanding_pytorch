{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \n",
    "# In this notebook, you learn:\n",
    "#\n",
    "# 1) How to use autograd with tensors without using any Neural Networks?\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2])\n",
      "tensor([1., 2.], requires_grad=True)\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# By passing the argument 'requires_grad=True', we are telling pytorch that\n",
    "# t1 is a learnable parameter.\n",
    "# Every tensor has a 'grad_fn' variable that references the function that created\n",
    "# the tensor except for the tensors created by the user (These tensors have\n",
    "# 'grad_fn' set to None).\n",
    "# t1 and t2 are user created tensors and hence have 'grad_fn' set to None.\n",
    "# t1 is also a leaf tensor (paramater tensor directly created by the user and \n",
    "# not an intermediate tensor created during the gradient calculation).\n",
    "# Gradients for the leaf tensors are saved in the '.grad' attribute.\n",
    "t1 = torch.tensor(data=[1, 2], dtype=torch.float, requires_grad=True)\n",
    "print(t1.shape)\n",
    "print(t1)\n",
    "print(t1.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2])\n",
      "tensor([-3., -4.], requires_grad=True)\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "t2 = torch.tensor(data=[-3, -4], dtype=torch.float, requires_grad=True)\n",
    "print(t2.shape)\n",
    "print(t2)\n",
    "print(t2.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2])\n",
      "tensor([-2., -2.], grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# t3 is created by the addition operation and hence grad_fn is set\n",
    "# to the appropriate Function object (AddBackward in this case).\n",
    "\n",
    "t3 = t1 + t2\n",
    "print(t3.shape)\n",
    "print(t3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2])\n",
      "tensor([-3., -8.], grad_fn=<MulBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# This was inferred as element wise multiplication by pytorch.\n",
    "# Notice the 'grad_fn' object here set to MulBackward appropriately.\n",
    "\n",
    "t4 = t1 * t2\n",
    "print(t4.shape)\n",
    "print(t4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2])\n",
      "tensor([1., 6.], grad_fn=<SubBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Notice that 'grad_fn' here only shows the last operation (meaning Function object \n",
    "# corresponding to that last operation) that was part of creating the new tensor 't5' \n",
    "# even though it contains multiple tensors (t3, t4) that were created with different \n",
    "# operations and have 'grad_fn' set on those tensors.\n",
    "\n",
    "t5 = t3 - t4\n",
    "print(t5.shape)\n",
    "print(t5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2])\n",
      "tensor([-6.,  8.], grad_fn=<SubBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Q = 3(a^3) - (b^2) --> tensor of shape 2 [q1, q2] = t6 below.\n",
    "# Loss = (q1 + q2) --> We use this as our loss to calculate the\n",
    "# gradients (t7 below).\n",
    "# We can also use Q as our Loss function and calculate the gradients. However,\n",
    "# Q is a tensor and we explicitly need to pass (info) whether we need to\n",
    "# calculate the derivatives based on every element in Q or only certain elements\n",
    "# which is cumbersome and not used in ML widely.\n",
    "\n",
    "t6 = 3*(t1**3) - (t2**2)\n",
    "print(t6.shape)\n",
    "print(t6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([])\n",
      "tensor(2., grad_fn=<SumBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Loss = (q1 + q2) --> We use this as our loss to calculate the\n",
    "# gradients (t6). It's a scalar value.\n",
    "\n",
    "Loss = t6.sum()\n",
    "print(Loss.shape)\n",
    "print(Loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Printing t1: \n",
      "tensor([1., 2.], requires_grad=True)\n",
      "tensor([ 9., 36.])\n",
      "Printing t2: \n",
      "tensor([-3., -4.], requires_grad=True)\n",
      "tensor([6., 8.])\n"
     ]
    }
   ],
   "source": [
    "# Triggering the back propogation calculates the gradients and saves them\n",
    "# in the grad attribute.\n",
    "\n",
    "Loss.backward()\n",
    "print(\"Printing t1: \")\n",
    "print(t1)\n",
    "print(t1.grad)\n",
    "print(\"Printing t2: \")\n",
    "print(t2)\n",
    "print(t2.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".pytorch_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
